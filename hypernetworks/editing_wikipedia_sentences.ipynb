{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run\n",
    "download_wikipedia.ipynb \n",
    "to get the file\n",
    "wikipedia_three_sentences.csv\n",
    "\n",
    "Then, we do our processing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_index=8\n",
    "num_editing_heads = 768*2 #more seems to be better for this\n",
    "editor_channel_width = 768 * 2\n",
    "max_grad_clip = 4.0\n",
    "chop_layer = 6\n",
    "lr = 1e-3\n",
    "edit_dampening_factor=1/100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch['tokenized_first_sentence'][30].size(), batch['tokenized_next_50_tokens'][30].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trying out the editor hypernetwork on the dune dataset\n",
    "import wandb\n",
    "\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"hypernetworks\",\n",
    "#     config={\"targetmodel\": \"gpt2\", \"editormodel\": \"gpt2\"},\n",
    "# )\n",
    "# # Copy this below where needed!\n",
    "# # run.log_model(path=\"<path-to-model>\", name=\"<name>\")\n",
    "\n",
    "# # wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# # wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# Set torch default device\n",
    "import torch\n",
    "# torch.set_default_device(\"cuda\")\n",
    "torch.set_default_device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2Model\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "import torch\n",
    "from torch import compile\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import contextlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cross_attention_to_layer(block, config):\n",
    "    block.crossattention = GPT2Attention(config, is_cross_attention=True)\n",
    "    block.ln_cross_attn = nn.LayerNorm(\n",
    "        normalized_shape=768, eps=config.layer_norm_epsilon\n",
    "    )\n",
    "    original_query_weights = block.attn.c_attn.weight[:, :768]\n",
    "    original_keys_values = block.attn.c_attn.weight[:, 768:]\n",
    "    original_query_bias = block.attn.c_attn.bias[:768]\n",
    "    original_keys_values_bias = block.attn.c_attn.bias[768:]\n",
    "    with torch.no_grad():\n",
    "        # Initialize the new layer with these parameters\n",
    "        block.crossattention.q_attn.weight = nn.Parameter(original_query_weights)\n",
    "        block.crossattention.q_attn.bias = nn.Parameter(original_query_bias)\n",
    "        block.crossattention.c_attn.weight = nn.Parameter(original_keys_values)\n",
    "        block.crossattention.c_attn.bias = nn.Parameter(original_keys_values_bias)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we stick on the \"reverse attention\" module at the end!\n",
    "This is a customized attention head that reads from the editor model, and writes to the target model's activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Editor_Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Controls whether the head will do a global softmax in all positions & layers\n",
    "        # If True, the attn is global and will sum to 1\n",
    "        # If False, the attn is a logistic fxn independently for every layer & token\n",
    "        # I suspect we will also want to penalize the intervention norm\n",
    "        self.num_editing_heads = (\n",
    "            config.num_editing_heads\n",
    "        )  # should default to 1, but we're going to test adding more\n",
    "        self.edit_channel_width = config.edit_channel_width\n",
    "        if self.edit_channel_width % self.num_editing_heads != 0:\n",
    "            print(\"Error: config hidden size is not divisible by num_editing_heads\")\n",
    "        self.head_dim = self.edit_channel_width // self.num_editing_heads\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        max_positions = (\n",
    "            config.max_position_embeddings\n",
    "        )  # does this do anything? can try killing this later\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(\n",
    "                torch.ones((max_positions, max_positions), dtype=torch.bool)\n",
    "            ).view(1, 1, max_positions, max_positions),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n",
    "\n",
    "        # We compute Q and K as a single nn.linear; but will later break apart into subcomponents\n",
    "\n",
    "        ## Before modification to a variable channel-width\n",
    "        # self.q_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.k_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.v_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        self.q_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.k_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.v_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.out_proj = nn.Linear(self.edit_channel_width, self.embed_dim)\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, head_dim).\"\"\"\n",
    "        new_shape = x.size()[:-1] + (self.num_editing_heads, self.head_dim)\n",
    "        return x.view(*new_shape)\n",
    "\n",
    "    def _new_reverse_attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        # Assume that we are doing softmax attention\n",
    "\n",
    "        print(\"QUERY SHAPE (target activations)\", query.shape)\n",
    "        print(\"KEY SHAPE (editor)\", key.shape)\n",
    "\n",
    "        # Project and split the query, key, value tensors\n",
    "        split_query = self._split_heads(query)\n",
    "        split_key = self._split_heads(key)\n",
    "        split_value = self._split_heads(value)\n",
    "\n",
    "        # Double-application (is this actually good/better for some reason?)\n",
    "        # self._split_heads(self.q_attn(query))\n",
    "        # self._split_heads(self.k_attn(key))\n",
    "        # self._split_heads(self.v_attn(value))\n",
    "\n",
    "        if split_query.dim() != 4:\n",
    "            print(\"Error: Expected query to be 4D tensor, but got something else!\")\n",
    "        if split_key.dim() != 3:\n",
    "            print(\"Error: Expected key to be 3D tensor, but got something else!\")\n",
    "        if split_value.dim() != 3:\n",
    "            print(\"Error: Expected value to be 3D tensor, but got something else!\")\n",
    "\n",
    "        # Query should be shaped as (batch_index, sequence_index, head_index, head_dim)\n",
    "        # Key and value should be shaped as (batch_index, head_index, head_dim)\n",
    "\n",
    "        KQ_weights = torch.matmul(\n",
    "            split_query.permute(0, 2, 1, 3), split_key.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Then we take the softmax within the positional divisions\n",
    "        softmaxed_weights = nn.functional.softmax(KQ_weights, dim=-1)\n",
    "\n",
    "        # Adjusting value selection for head dimension\n",
    "        attn_output = torch.matmul(\n",
    "            softmaxed_weights.unsqueeze(-1), split_value.unsqueeze(-2)\n",
    "        )\n",
    "\n",
    "        # combine heads: change 50, 8, 104, 96 to 50, 104, 768\n",
    "        # first, permute\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3)\n",
    "        # combin heads x head_dims\n",
    "        attn_output = attn_output.reshape(\n",
    "            -1, attn_output.size(1), attn_output.size(2) * attn_output.size(3)\n",
    "        )\n",
    "        # now project back\n",
    "        projected_output = self.out_proj(attn_output)\n",
    "\n",
    "        return projected_output, softmaxed_weights\n",
    "\n",
    "    def _reverse_attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        if key.dim() == 4:\n",
    "            K_reduced = key[\n",
    "                :, :, -1, :\n",
    "            ]  # R# Check: that the second dimension of K is only a single element when we have batching\n",
    "            KQ_weights = torch.bmm(K_reduced, query.transpose(1, 2))\n",
    "            logistic_weights = torch.atan(KQ_weights)\n",
    "            attn_output = torch.bmm(\n",
    "                logistic_weights.transpose(1, 2),\n",
    "                value[\n",
    "                    :, :, -1, :\n",
    "                ],  # we take the editor output only over the final token position\n",
    "            )\n",
    "\n",
    "        if key.dim() == 3:\n",
    "            QK_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "            logistic_weights = torch.atan(QK_weights)\n",
    "            attn_output = torch.matmul(logistic_weights, value)\n",
    "\n",
    "        return attn_output, logistic_weights\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        editor_hidden_states,\n",
    "        target_hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # Here, the query is the target hidden encoder, the key is the editor, and the value is the editor\n",
    "        query = self.q_attn(target_hidden_states)\n",
    "        if editor_hidden_states.dim() == 3:\n",
    "            key = self.k_attn(\n",
    "                # I don't quite understand why sometimes editor_hidden_states is 4 dimensional, sometimes 3\n",
    "                # seems like it's sometimes 20, 1, 4, 768 and sometimes 20, 4, 768. what gives?\n",
    "                editor_hidden_states[:, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "            value = self.v_attn(\n",
    "                # [:, 0, :1, :]\n",
    "                editor_hidden_states[:, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "\n",
    "        if editor_hidden_states.dim() == 4:\n",
    "            key = self.k_attn(\n",
    "                editor_hidden_states[:, 0, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "            value = self.v_attn(\n",
    "                # [:, 0, :1, :]\n",
    "                editor_hidden_states[:, 0, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "\n",
    "        attn_output, attn_weights = self._new_reverse_attn(query, key, value)\n",
    "\n",
    "        if output_attentions:\n",
    "            return attn_output, attn_weights\n",
    "        else:\n",
    "            return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "\n",
    "def new_forward(\n",
    "    self,\n",
    "    input_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "    attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    token_type_ids: Optional[torch.LongTensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    head_mask: Optional[torch.FloatTensor] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "    encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    # labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    ") -> Union[Tuple]:\n",
    "    r\"\"\"\n",
    "    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "        Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "        `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "        are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "    \"\"\"\n",
    "\n",
    "    transformer_outputs = self.transformer(\n",
    "        input_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids,\n",
    "        head_mask=head_mask,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        encoder_attention_mask=encoder_attention_mask,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    hidden_states = transformer_outputs[0]\n",
    "\n",
    "    # Set device for model parallelism\n",
    "    if self.model_parallel and torch.cuda.is_available():\n",
    "        torch.cuda.set_device(self.transformer.first_device)\n",
    "        hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "    # lm_logits = self.lm_head(hidden_states)\n",
    "    reverse_attention_output = self.lm_head(\n",
    "        hidden_states, encoder_hidden_states, output_attentions=output_attentions\n",
    "    )\n",
    "\n",
    "    return reverse_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_final_layer_with_bespoke_reverse_attention(model):\n",
    "    model.lm_head = Editor_Attention(config=model.config)\n",
    "    model.forward = new_forward.__get__(model, GPT2LMHeadModel)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def add_fwd_hooks(module_hooks):\n",
    "    \"\"\"\n",
    "    Context manager for temporarily adding forward hooks to a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    module_hooks\n",
    "        A list of pairs: (module, fnc) The function will be registered as a\n",
    "            forward hook on the module\n",
    "    \"\"\"\n",
    "    try:\n",
    "        handles = []\n",
    "        for mod, hk in module_hooks:\n",
    "            handles.append(mod.register_forward_hook(hk))\n",
    "        yield\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_layer_indices(model):\n",
    "    \"\"\"\n",
    "    Assigns a custom attribute 'layer_index' to each transformer layer in the GPT-2 model.\n",
    "    This function iterates over the transformer blocks and assigns an index to each.\n",
    "    \"\"\"\n",
    "    model.transformer.wte.layer_index = 0\n",
    "    for i, layer in enumerate(model.transformer.h):\n",
    "        layer.layer_index = i + 1\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# assign_layer_indices(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_and_pad(A, B):\n",
    "    batch_size, n_tokens_A = A.size()\n",
    "    n_tokens_B = B.size(1)\n",
    "\n",
    "    # Concatenate A and B along dimension 1\n",
    "    concatenated = torch.cat((A, B), dim=1)\n",
    "    \n",
    "    # Find the number of non-zero elements in each row of concatenated tensor\n",
    "    lengths = torch.sum(concatenated != 50256, dim=1)\n",
    "    \n",
    "    # Create a mask to identify the non-zero elements in concatenated tensor\n",
    "    mask = torch.arange(n_tokens_A + n_tokens_B).expand(batch_size, n_tokens_A + n_tokens_B) < lengths.unsqueeze(1)\n",
    "    \n",
    "    # Create a new tensor with the same shape as concatenated tensor\n",
    "    result = torch.full_like(concatenated, 50256)\n",
    "    \n",
    "    # Move the non-zero elements to the left and zero elements to the right\n",
    "    result[mask] = concatenated[mask]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan_gradients(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and torch.isnan(param.grad).any():\n",
    "            print(f\"NaN values found in gradient of parameter: {name}\")\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.compile #Apparently this fails when used inside jupyter notebooks but is fine if i make dedicated scripts\n",
    "class EditorHypernetwork(nn.Module):\n",
    "    # Separating the editor config file, from its base model's configurations\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_editing_heads=32,\n",
    "        edit_channel_width=768,  # controls dimensionality given to attention heads in the last layer of the editor\n",
    "        use_layerwise_embeddings=True,\n",
    "        chop_editor_at_layer=None,\n",
    "        edit_dampening_factor=0.001,  # tuning parameter to help the edits not be initialized too large\n",
    "        kill_token_zero=False,  # multiplies edits to token pos zero by zero\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Construct Editor Model\n",
    "        # Load the configuration from the YAML file\n",
    "        # with open(editor_yaml_file_path, 'r') as file:\n",
    "        #     self.config = yaml.safe_load(file)\n",
    "        if torch.cuda.is_available():\n",
    "            self.editor_model = (\n",
    "                GPT2LMHeadModel.from_pretrained(\"gpt2\").cuda().eval()\n",
    "            )  # have recently added .cuda() so it uses the gpu\n",
    "        else:\n",
    "            self.editor_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"mps\").eval()\n",
    "\n",
    "        # Add cross-attention to each layer\n",
    "        self.editor_model.config.add_cross_attention = True\n",
    "        self.editor_model.config.num_editing_heads = num_editing_heads\n",
    "        self.editor_model.config.chop_layer = chop_editor_at_layer\n",
    "        self.editor_model.config.kill_token_zero = kill_token_zero\n",
    "        self.editor_model.config.edit_channel_width = edit_channel_width\n",
    "\n",
    "        if chop_editor_at_layer is None:\n",
    "            chop_editor_at_layer = 12\n",
    "\n",
    "        for i in range(chop_editor_at_layer):\n",
    "            add_cross_attention_to_layer(\n",
    "                self.editor_model.transformer.h[i], self.editor_model.config\n",
    "            )\n",
    "\n",
    "        # Delete extra layers beyond the chop_layer\n",
    "        self.editor_model.transformer.h = self.editor_model.transformer.h[\n",
    "            :chop_editor_at_layer\n",
    "        ]\n",
    "\n",
    "        # Replace the final linear layer with special reverse attention output\n",
    "        self.editor_model.lm_head = Editor_Attention(config=self.editor_model.config)\n",
    "        self.editor_model.forward = new_forward.__get__(\n",
    "            self.editor_model, GPT2LMHeadModel\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            self.editor_model.cuda()\n",
    "        else:\n",
    "            self.editor_model.to(\"mps\")\n",
    "\n",
    "        # Construct Target Model\n",
    "        if torch.cuda.is_available():\n",
    "            self.target_model = (\n",
    "                transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\").cuda().eval()\n",
    "            )\n",
    "        else:\n",
    "            self.target_model = (\n",
    "                transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "                .to(\"mps\")\n",
    "                .eval()\n",
    "            )\n",
    "        for param in self.target_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        assign_layer_indices(self.target_model)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.target_model.cuda()\n",
    "        else:\n",
    "            self.target_model.to(\"mps\")\n",
    "\n",
    "        # Add module for layerwise embeddings\n",
    "        if use_layerwise_embeddings:\n",
    "            self.use_layerwise_embeddings = True\n",
    "            self.layerwise_embeddings = torch.randn(13, 768, requires_grad=True).to(\n",
    "                \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "            )\n",
    "        else:\n",
    "            self.use_layerwise_embeddings = False\n",
    "            self.layerwise_embeddings = 0\n",
    "\n",
    "        self.edit_dampening_factor = edit_dampening_factor\n",
    "\n",
    "        self.residual_cache = None\n",
    "        self.opt = None\n",
    "        self.lossfn = None\n",
    "        self.lam = None\n",
    "        self.penalty_loss = None\n",
    "        self.training_loss = None\n",
    "\n",
    "    # Gets the hidden states from the target model, if necessary\n",
    "    def run_target_model_for_encoded_hidden_states(self, target_ids):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.target_model(target_ids, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            return hidden_states\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        editor_input_ids,\n",
    "        target_input_ids,\n",
    "        target_hidden_states=None,\n",
    "        output_target_hidden_states=False,\n",
    "        output_edited_hidden_states=False,\n",
    "        output_edit_vectors=False,\n",
    "        output_editor_attention=False,\n",
    "        stop_editing_index=None,\n",
    "        batch_edit_vectors=None,\n",
    "    ):\n",
    "        # Run target model for encoded hidden states\n",
    "        if target_hidden_states is None:\n",
    "            target_hidden_states = torch.stack(\n",
    "                self.run_target_model_for_encoded_hidden_states(\n",
    "                    target_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "                ),  # seems to break while we are passing thru batch_size=1; the last (12th =) has different dimensions\n",
    "                dim=2,\n",
    "            )\n",
    "        # dimensions of target_hidden_states:\n",
    "        # batch_size, token_sequence_length, num_layers = 13, resid_width = 768\n",
    "\n",
    "        # If we are stopping editing at stop_editing_index, then we eliminate target_hidden_states beyond that index\n",
    "        if stop_editing_index is not None:\n",
    "            target_hidden_states = target_hidden_states[\n",
    "                :, :stop_editing_index, :, :\n",
    "            ].clone()\n",
    "\n",
    "        # Normalize along the last dimension\n",
    "        normalization_factors = target_hidden_states.norm(dim=-1, keepdim=True)\n",
    "        target_hidden_states = target_hidden_states / normalization_factors\n",
    "\n",
    "        # Error catching:\n",
    "        if batch_edit_vectors is not None:\n",
    "            if output_edit_vectors or output_editor_attention:\n",
    "                return \"Error: Inputting your own batch_edit_vectors means the model does not construct the outputs you are requesting\"\n",
    "\n",
    "        # Run editor model, get edit vectors\n",
    "        if batch_edit_vectors is None:\n",
    "            if self.use_layerwise_embeddings:\n",
    "                # Now, add in the layerwise embeddings\n",
    "                embedded_hidden_states = (\n",
    "                    target_hidden_states + self.layerwise_embeddings[None, None, :, :]\n",
    "                )\n",
    "\n",
    "                collapsed_target_hidden_states = embedded_hidden_states.reshape(\n",
    "                    target_hidden_states.shape[0],\n",
    "                    target_hidden_states.shape[1] * target_hidden_states.shape[2],\n",
    "                    target_hidden_states.shape[3],\n",
    "                )\n",
    "            else:\n",
    "                collapsed_target_hidden_states = target_hidden_states.reshape(\n",
    "                    target_hidden_states.shape[0],\n",
    "                    target_hidden_states.shape[1] * target_hidden_states.shape[2],\n",
    "                    target_hidden_states.shape[3],\n",
    "                )\n",
    "\n",
    "            editor_output = self.editor_model(\n",
    "                editor_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\"),\n",
    "                encoder_hidden_states=collapsed_target_hidden_states,\n",
    "                output_attentions=output_editor_attention,\n",
    "            )\n",
    "            # Multiply the outputs by normalization factors\n",
    "            if output_editor_attention:\n",
    "                temp_edit_vectors = editor_output[0]\n",
    "                # Might want to reshape this too but whatever\n",
    "                batch_editor_attention = editor_output[1]\n",
    "            else:\n",
    "                temp_edit_vectors = editor_output\n",
    "\n",
    "            # Renormalize to the scale of the target hidden states\n",
    "            # and reshape to proper dimensions\n",
    "            batch_edit_vectors = (\n",
    "                self.edit_dampening_factor\n",
    "                * normalization_factors\n",
    "                * temp_edit_vectors.reshape(\n",
    "                    temp_edit_vectors.shape[0], stop_editing_index, 13, 768\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # If we are stopping editing at stop_editing_index,\n",
    "        # this pads batch_edit_vectors with 0's to the right of the edited positions\n",
    "        if stop_editing_index is not None:\n",
    "            batch_edit_vectors = torch.cat(\n",
    "                (\n",
    "                    batch_edit_vectors,\n",
    "                    torch.zeros(\n",
    "                        batch_edit_vectors.shape[0],\n",
    "                        target_input_ids.shape[1] - stop_editing_index,\n",
    "                        13,\n",
    "                        768,\n",
    "                    ),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        # Run target model with edit vectors. This adds the edit vectors to the given hidden state at the specified batch index, position, and layer\n",
    "        def edit_add(module, input, output):\n",
    "            layer_index = module.layer_index\n",
    "            output[0][:] = output[0] + batch_edit_vectors[:, :, layer_index, :]\n",
    "            if self.editor_model.config.kill_token_zero == True:\n",
    "                output[0][:, 0, :] = 0\n",
    "\n",
    "        def embedding_edit_add(module, input, output):\n",
    "            output[:] = output + batch_edit_vectors[:, :, 0, :]\n",
    "            if self.editor_model.config.kill_token_zero == True:\n",
    "                output[:, 0, :] = 0\n",
    "\n",
    "        # Now editing the target model\n",
    "        hooks1 = [(self.target_model.transformer.wte, embedding_edit_add)]\n",
    "        hooks2 = [(self.target_model.transformer.h[L], edit_add) for L in range(12)]\n",
    "        hooks = hooks1 + hooks2\n",
    "        with add_fwd_hooks(hooks):\n",
    "            # THIS IS THE LINE WHERE THE MODEL IS CALLED (AND THE EDITOR IS CALLED AT\n",
    "            # THE END OF `layer` AS A SIDE EFFECT)\n",
    "            target_result = self.target_model(\n",
    "                target_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\"),\n",
    "                output_hidden_states=output_edited_hidden_states,\n",
    "            )\n",
    "\n",
    "        logits = target_result.logits\n",
    "\n",
    "        output = {}\n",
    "        output[\"logits\"] = logits\n",
    "        if output_target_hidden_states:\n",
    "            output[\"target_hidden_states\"] = (\n",
    "                target_hidden_states * normalization_factors\n",
    "            )\n",
    "        if output_edited_hidden_states:\n",
    "            output[\"edited_hidden_states\"] = target_result.hidden_states\n",
    "        if output_edit_vectors:\n",
    "            output[\"edit_vectors\"] = batch_edit_vectors\n",
    "        if output_editor_attention:\n",
    "            output[\"editor_attention\"] = batch_editor_attention\n",
    "        return output\n",
    "\n",
    "    # Generate text using the target model, with a new edit application at every step.\n",
    "    # This is a very slow way to generate text.\n",
    "    # If you only want to edit first k tokens, use the forward pass instead with stop_editing_index = k\n",
    "    def inspect_batch_prediction_ouptuts(self, batch):\n",
    "        with torch.no_grad():\n",
    "            batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "            self.editor_inputs = batch[\"tokenized_first_sentence\"][i].unsqueeze(0)\n",
    "            self.target_inputs = batch[\"tokenized_next_50_tokens\"][i].unsqueeze(0)\n",
    "            self.prediction = self.forward(\n",
    "                self.editor_inputs,\n",
    "                self.target_inputs,\n",
    "                stop_editing_index=None,\n",
    "                output_target_hidden_states=False,\n",
    "                output_edited_hidden_states=False,\n",
    "                output_edit_vectors=False,\n",
    "                output_editor_attention=False,\n",
    "            )\n",
    "            # compute most likely tokens from the logits\n",
    "            predicted_ids = [\n",
    "                torch.argmax(pred[\"logits\"], dim=-1) for pred in predictions\n",
    "            ]\n",
    "            # convert the token ids to strings\n",
    "            predicted_strings = [tokenizer.decode(pred) for pred in predicted_ids]\n",
    "            return predicted_strings\n",
    "\n",
    "    def evaluate_KL_test_loss_nogradient(\n",
    "        self, dataloader, f_data_to_soft_labels=None, stop_editing_index=8\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            sum_weighted_losses = 0.0\n",
    "            total_samples = 0\n",
    "            for batch in dataloader:\n",
    "                current_batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "                self.editor_inputs = batch[\"tokenized_first_sentence\"].squeeze(1)\n",
    "                self.target_inputs = batch[\"tokenized_next_50_tokens\"].squeeze(1)\n",
    "                self.prediction = self.forward(  # check the batch size\n",
    "                    self.editor_inputs,\n",
    "                    self.target_inputs,\n",
    "                    stop_editing_index=stop_editing_index,\n",
    "                )\n",
    "                log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                    self.prediction[\"logits\"][:, stop_editing_index:, :].reshape(\n",
    "                        -1, 50257\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "                # Now we must compute the soft labels!\n",
    "                # join the last 50 tokens to the editor inputs\n",
    "                soft_labels = f_data_to_soft_labels(\n",
    "                    batch[\"tokenized_first_sentence\"],\n",
    "                    batch[\"tokenized_next_50_tokens\"],\n",
    "                    num_predictions_max=50,\n",
    "                )\n",
    "                mask = (batch[\"tokenized_next_50_tokens\"] != 50256).reshape(-1)\n",
    "                self.loss = torch.nn.functional.kl_div(\n",
    "                    log_prob_predictions, soft_labels, reduction=\"batchmean\"\n",
    "                )\n",
    "                # Weight the loss by current batch size and update the sum of weighted losses\n",
    "                sum_weighted_losses += self.loss.item() * current_batch_size\n",
    "                total_samples += current_batch_size\n",
    "            weighted_average_loss = sum_weighted_losses / total_samples\n",
    "        return weighted_average_loss\n",
    "\n",
    "    def evaluate_crossentropy_test_loss_nogradient(\n",
    "        self, dataloader, stop_editing_index=8\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            sum_weighted_losses = 0.0\n",
    "            total_samples = 0\n",
    "            for batch in dataloader:\n",
    "                # current_batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "                self.editor_inputs = batch[\"tokenized_first_sentence\"].squeeze(1)\n",
    "                self.target_inputs = batch[\"tokenized_next_50_tokens\"].squeeze(1)\n",
    "                self.prediction = self.forward(  # check the batch size\n",
    "                    self.editor_inputs,\n",
    "                    self.target_inputs,\n",
    "                    stop_editing_index=stop_editing_index,\n",
    "                )\n",
    "                log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                    self.prediction[\"logits\"][:, stop_editing_index:, :].reshape(\n",
    "                        -1, 50257\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "                # Create a mask to exclude padded tokens\n",
    "                target_labels = self.target_inputs[:, stop_editing_index:].reshape(-1)\n",
    "                mask = (\n",
    "                    target_labels != 50256\n",
    "                )  # Assuming padded tokens are represented by 0\n",
    "\n",
    "                # Compute the cross-entropy loss with masking\n",
    "                criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                loss = criterion(log_prob_predictions, target_labels)\n",
    "                current_mask_sum = mask.sum()\n",
    "                loss = (loss * mask).sum() / current_mask_sum\n",
    "\n",
    "                # Weight the loss by current batch size and update the sum of weighted losses\n",
    "                sum_weighted_losses += loss * current_mask_sum\n",
    "                total_tokens += curr_mask_sum\n",
    "            weighted_average_loss = sum_weighted_losses / total_tokens\n",
    "        return weighted_average_loss\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader,\n",
    "        test_loader=None,\n",
    "        stop_editing_index=8,\n",
    "        epochs=1,\n",
    "        KL_divergence_loss=False,\n",
    "        lam=0,  # 20000\n",
    "        lam_testing_penalty=0,  # 100000\n",
    "        f_data_to_soft_labels=None,\n",
    "        checkpoint_interval=60,  # save checkpoint every 60 minutes\n",
    "    ):\n",
    "        self.opt = optim.AdamW(\n",
    "            self.parameters(), lr=lr, weight_decay=0.01\n",
    "        )  # usually: lr = 5e-5. 1e-3 worked well!\n",
    "\n",
    "        if KL_divergence_loss:\n",
    "            self.lossfn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        else:\n",
    "            self.lossfn = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Create a tqdm progress bar\n",
    "            with tqdm(\n",
    "                total=len(train_loader),\n",
    "                desc=f\"Epoch {epoch + 1}/{epochs}\",\n",
    "                unit=\"batch\",\n",
    "            ) as pbar:\n",
    "                num_datapoints_in_epoch = 0\n",
    "                epoch_train_loss = 0\n",
    "                epoch_gradient_norm = 0\n",
    "                # Train loop\n",
    "                batch_index = -1  # index of first batch will be 0\n",
    "\n",
    "                for batch in (\n",
    "                    train_loader\n",
    "                ):  # not sure what this does for fractional batches. meh whatev\n",
    "                    batch_index += 1\n",
    "                    self.batch = batch\n",
    "                    current_batch_size = len(batch[\"tokenized_next_50_tokens\"])\n",
    "                    num_datapoints_in_epoch += current_batch_size\n",
    "                    self.opt.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    self.prediction = self.forward(\n",
    "                        batch[\"tokenized_first_sentence\"],\n",
    "                        batch[\"tokenized_next_50_tokens\"],\n",
    "                        stop_editing_index=stop_editing_index,\n",
    "                        output_target_hidden_states=True,\n",
    "                        output_edited_hidden_states=True,\n",
    "                        output_edit_vectors=True,\n",
    "                    )\n",
    "\n",
    "                    # Compute the penalty (edit size relative to the hidden state)\n",
    "                    self.lam = lam\n",
    "                    edit_ratio = self.prediction[\"edit_vectors\"].norm(dim=-1)[\n",
    "                        :, :stop_editing_index, :\n",
    "                    ] / self.prediction[\"target_hidden_states\"].norm(dim=-1)\n",
    "                    self.per_datapoint_penalty_loss = self.lam * torch.sum(\n",
    "                        edit_ratio, dim=[1, 2]\n",
    "                    )\n",
    "                    self.penalty_loss = torch.mean(self.per_datapoint_penalty_loss)\n",
    "\n",
    "                    # Compute the data loss\n",
    "                    if KL_divergence_loss:\n",
    "                        log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                            self.prediction[\"logits\"][:, :, :].reshape(-1, 50257),\n",
    "                            dim=1,\n",
    "                        )\n",
    "                        # Now we must compute the soft labels! This is outsourced to the user-provided function, teacher_model\n",
    "                        self.soft_labels = f_data_to_soft_labels(\n",
    "                            batch[\"tokenized_first_sentence\"],\n",
    "                            batch[\"tokenized_next_50_tokens\"],\n",
    "                            num_predictions_max=50,\n",
    "                        )\n",
    "                        # check that the mask makes sense!\n",
    "                        mask = (batch[\"tokenized_next_50_tokens\"] != 50256).reshape(-1)\n",
    "                        self.prediction_loss = self.lossfn(\n",
    "                            log_prob_predictions[mask, :], self.soft_labels[mask, :]\n",
    "                        )\n",
    "                        # NOTE: currently I am letting the loss predict on tokens inside the editing window\n",
    "                        # I didn't do this in the previous testing! Nor is it the case in crossentropy\n",
    "\n",
    "                    else:\n",
    "                        log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                            self.prediction[\"logits\"][\n",
    "                                :, stop_editing_index:, :\n",
    "                            ].reshape(-1, 50257),\n",
    "                            dim=1,\n",
    "                        )\n",
    "                        # Create a mask to exclude padded tokens\n",
    "                        # NOTE: here we are disallowing prediction on the first stop_editing_index tokens.\n",
    "                        # Code is currently formatted such that this is not the case for KL\n",
    "                        target_labels = batch[\"tokenized_next_50_tokens\"][\n",
    "                            :, stop_editing_index:\n",
    "                        ].reshape(-1)\n",
    "                        mask = target_labels != 50256\n",
    "                        # Compute the cross-entropy loss with masking\n",
    "                        criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                        loss = criterion(log_prob_predictions, target_labels.long())\n",
    "                        current_mask_sum = mask.sum()\n",
    "                        self.prediction_loss = (loss * mask).sum() / current_mask_sum\n",
    "\n",
    "                    # Compute the total loss and backpropagate\n",
    "                    self.training_loss = self.prediction_loss + self.penalty_loss\n",
    "                    self.training_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(\n",
    "                        self.parameters(), max_grad_clip\n",
    "                    )  # just implemented this! dunno if a cap of 1 to large, so I'm messing with reducing it\n",
    "\n",
    "                    # Check for nan gradients\n",
    "                    # if check_nan_gradients(self):\n",
    "                    #     break\n",
    "\n",
    "                    # Backwards step\n",
    "                    self.opt.step()\n",
    "\n",
    "                    # metrics\n",
    "                    epoch_train_loss += self.training_loss.item() * current_batch_size\n",
    "                    gradients = [\n",
    "                        p.grad.view(-1) for p in self.parameters() if p.grad is not None\n",
    "                    ]\n",
    "                    all_gradients = torch.cat(gradients)\n",
    "                    gradient_norm = torch.norm(all_gradients).item()\n",
    "                    epoch_gradient_norm += gradient_norm * current_batch_size\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"train_batch_total_loss\": self.training_loss.item(),\n",
    "                            \"train_batch_prediction_loss\": self.prediction_loss.item(),\n",
    "                            \"train_batch_penalty_loss\": self.penalty_loss,\n",
    "                            \"train_batch_gradient_norm\": gradient_norm,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.update(1)  # note: this was incorrectly displaying before!\n",
    "\n",
    "                    # Check if it's time to save a checkpoint\n",
    "                    current_time = time.time()\n",
    "                    # first loop initialization\n",
    "                    if batch_index == 0 and epoch == 0:\n",
    "                        last_checkpoint_time = -100000\n",
    "\n",
    "                    if current_time - last_checkpoint_time >= checkpoint_interval * 60:\n",
    "                        # Save the checkpoint\n",
    "                        torch.save(\n",
    "                            self.state_dict(),\n",
    "                            f\"checkpoint_epoch_{epoch}_batch_{pbar.n}.pt\",\n",
    "                        )\n",
    "                        last_checkpoint_time = current_time\n",
    "                        # announce checkpoint save\n",
    "                        print(\"Checkpoint saved at epoch\", epoch, \"batch\", pbar.n)\n",
    "\n",
    "                ####END BATCH LOOP\n",
    "                #########################\n",
    "\n",
    "                # epoch loss\n",
    "                # epoch_test_prediction_loss = self.evaluate_crossentropy_test_loss_nogradient(\n",
    "                #     test_loader,\n",
    "                #     stop_editing_index,\n",
    "                #     batch_size\n",
    "                # )\n",
    "                if KL_divergence_loss:\n",
    "                    epoch_test_prediction_loss = self.evaluate_KL_test_loss_nogradient(\n",
    "                        dataloader=test_loader,\n",
    "                        f_data_to_soft_labels=f_data_to_soft_labels,\n",
    "                        stop_editing_index=stop_editing_index,\n",
    "                    )\n",
    "\n",
    "                # # Calculate and accumulate gradient norm for logging\n",
    "                # gradients = [p.grad.view(-1) for p in self.parameters() if p.grad is not None]\n",
    "                # all_gradients = torch.cat(gradients)\n",
    "                # gradient_norm = torch.norm(all_gradients).item()\n",
    "                # epoch_gradient_norm += gradient_norm\n",
    "\n",
    "                if wandb.run:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"epoch_train_total_loss\": epoch_train_loss\n",
    "                            / num_datapoints_in_epoch,\n",
    "                            \"test_prediction_loss\": epoch_test_prediction_loss,\n",
    "                            \"gradient_norm\": epoch_gradient_norm\n",
    "                            / num_datapoints_in_epoch,\n",
    "                        }\n",
    "                    )\n",
    "            # Save the final model\n",
    "            torch.save(self.state_dict(), \"final_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####DIVISION!\n",
    "#DATASET CONSTRUCTION BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the CSV file into a DataFrame\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "csv_dir = \"wikipedia_three_sentences.csv\"\n",
    "dataframe_dir = \"wikipedia_df.pt\"\n",
    "\n",
    "\n",
    "def tokenize_followup(tokenizer, row):\n",
    "    # Compose second_sentences and third_sentences\n",
    "    followup_text = row[\"second_sentences\"] + \" \" + row[\"third_sentences\"]\n",
    "\n",
    "    # Select the first 500 characters\n",
    "    followup_text = followup_text[:500]\n",
    "\n",
    "    # Tokenize the followup text\n",
    "    tokenized_followup = tokenizer.encode(followup_text)\n",
    "\n",
    "    # Check if the resulting tokenized list is still less than 50 tokens\n",
    "    if len(tokenized_followup) < 50:\n",
    "        # Pad with token 50256 to reach a length of 50 tokens\n",
    "        tokenized_followup = tokenized_followup + [50256] * (\n",
    "            50 - len(tokenized_followup)\n",
    "        )\n",
    "\n",
    "    return tokenized_followup\n",
    "\n",
    "\n",
    "def create_dataframe_from_csv(csv_dir):\n",
    "    df = pd.read_csv(csv_dir)\n",
    "\n",
    "    # # Printng out examples of longest and shortest entries\n",
    "    # # Iterate over each column\n",
    "    # for column in df.columns:\n",
    "    #     print(f\"Column: {column}\")\n",
    "\n",
    "    #     # Find the 10 longest entries in the column\n",
    "    #     longest_entries = df[column].astype(str).apply(len).nlargest(10)\n",
    "    #     print(\"Longest entries:\")\n",
    "    #     for index, length in longest_entries.items():\n",
    "    #         print(f\"Length: {length}, Entry: {df.loc[index, column]}\")\n",
    "\n",
    "    #     # Find the 10 shortest entries in the column\n",
    "    #     shortest_entries = df[column].astype(str).apply(len).nsmallest(10)\n",
    "    #     print(\"Shortest entries:\")\n",
    "    #     for index, length in shortest_entries.items():\n",
    "    #         print(f\"Length: {length}, Entry: {df.loc[index, column]}\")\n",
    "\n",
    "    #     print()\n",
    "\n",
    "    # Length of the dataframe to start\n",
    "    # print(f\"Length of dataframe: {len(df)}\")\n",
    "    # df.columns\n",
    "    # Filter the dataset based on sentence length\n",
    "\n",
    "    df[\"first_sentence_length\"] = df[\"first_sentences\"].apply(lambda x: len(x))\n",
    "    df[\"second_sentence_length\"] = df[\"second_sentences\"].apply(lambda x: len(x))\n",
    "    df[\"third_sentence_length\"] = df[\"third_sentences\"].apply(lambda x: len(x))\n",
    "\n",
    "    df_filtered = df[\n",
    "        (df[\"first_sentence_length\"] >= 5)\n",
    "        & (df[\"second_sentence_length\"] >= 10)\n",
    "        & (df[\"first_sentence_length\"] <= 100)\n",
    "    ]\n",
    "    # print(f\"Length of filtered dataframe: {len(df_filtered)}\")\n",
    "\n",
    "    from transformers import GPT2Tokenizer\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    df_filtered[\"tokenized_first_sentence\"] = df_filtered.apply(\n",
    "        lambda row: tokenizer.encode(row[\"first_sentences\"], add_special_tokens=False),\n",
    "        axis=1,\n",
    "    )\n",
    "    df_filtered[\"tokenized_next_50_tokens\"] = df_filtered.apply(\n",
    "        lambda row: partial(tokenize_followup, tokenizer)(row), axis=1\n",
    "    )\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "if os.path.exists(\"wikipedia_df.pt\"):\n",
    "    df = pd.read_pickle(\"wikipedia_df.pt\")\n",
    "else:  # save dataset\n",
    "    df = create_dataframe_from_csv(csv_dir)\n",
    "    df.to_pickle(\"wikipedia_df.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    res = {}\n",
    "    for k in batch[0].keys():\n",
    "        if k != '__index_level_0__':\n",
    "            els = [x[k] for x in batch]\n",
    "            max_length = max(len(x) for x in els)\n",
    "            if k == 'tokenized_next_50_tokens':\n",
    "                max_length = 50\n",
    "                for i in range(len(els)):\n",
    "                    if len(els[i]) > 50:\n",
    "                        els[i] = els[i][:50]\n",
    "            # for x in els:\n",
    "            #     x += [0] * (max_length - len(x))\n",
    "            #res[k] = torch.cat([y , torch.zeros(max_length - len(x))],for y in els)\n",
    "            #res[k]=torch.tensor([x + [0] * (max_length( - len(x)) for x in els], dtype=torch.long)\n",
    "            res[k]=torch.stack([ torch.cat((torch.tensor(x),torch.zeros((max_length - len(x))))) for x in els]).int()\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokenized_first_sentence', 'tokenized_next_50_tokens', '__index_level_0__'],\n",
       "        num_rows: 987633\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokenized_first_sentence', 'tokenized_next_50_tokens', '__index_level_0__'],\n",
       "        num_rows: 85882\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert 2 columns to Dataset and train-test split\n",
    "dataset = Dataset.from_pandas(df[['tokenized_first_sentence', 'tokenized_next_50_tokens']]).shuffle(seed=42)\n",
    "test_ratio = .08\n",
    "temp = dataset.train_test_split(test_size=test_ratio)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32 #50 or so\n",
    "data_loader = DataLoader(temp[\"train\"], batch_size=batch_size, collate_fn = collate_fn)#batch_size, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(temp[\"test\"], batch_size=batch_size, collate_fn = collate_fn)#batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# # Now you can iterate over data_loader in your training loop\n",
    "# j=0\n",
    "# for batch in data_loader:\n",
    "#     j=j+1\n",
    "# print(j)\n",
    "# #Good! we have all the data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['tokenized_first_sentence', 'tokenized_next_50_tokens', '__index_level_0__'],\n",
       "     num_rows: 1073515\n",
       " }),\n",
       " 32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, data_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x2e6ecf0d0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in data_loader:\n",
    "#     temp = batch\n",
    "#     break\n",
    "# temp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in data_loader:\n",
    "#     temp = batch\n",
    "#     break\n",
    "# # temp[\"tokenized_next_50_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_data_to_soft_labels(A, B, num_predictions_max = 50):\n",
    "    with torch.no_grad():\n",
    "        # Compute number of nonzero elements in each row of A and B\n",
    "        lengths_A = torch.sum(A != 50256, dim=1)\n",
    "        lengths_B = torch.sum(B != 50256, dim=1)\n",
    "        # Concatenate A and B along dimension 1\n",
    "        data = concat_and_pad(A, B) \n",
    "        logits = model(data).logits\n",
    "        predictions = torch.nn.functional.softmax(logits, dim=2)\n",
    "\n",
    "        #Create an empty tensor to store the predictions\n",
    "        shape=( len(lengths_A), num_predictions_max, 50257)\n",
    "        hold_output = torch.full(shape, torch.nan)\n",
    "\n",
    "        # Extract the predictions corresponding to B\n",
    "        for i in range(len(lengths_A)):\n",
    "            hold_output[i, :lengths_B[i], :] = predictions[i, lengths_A[i]:lengths_A[i] + lengths_B[i], :]\n",
    "\n",
    "        return hold_output.reshape(-1, 50257) #returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_editing_index = 8\n",
    "hypernetwork = EditorHypernetwork(\n",
    "    edit_dampening_factor = edit_dampening_factor, #1/10000, \n",
    "    use_layerwise_embeddings=False,\n",
    "    num_editing_heads=num_editing_heads,\n",
    "    edit_channel_width=editor_channel_width,\n",
    "    chop_editor_at_layer=chop_layer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for batch in data_loader:\n",
    "    if i == 18:\n",
    "        temp = batch\n",
    "        break\n",
    "    i += 1\n",
    "temp[\"tokenized_next_50_tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sample = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokenized_first_sentence', 'tokenized_next_50_tokens'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unto Us is the sixth studio album by Aaron Shust.!!!!!!!!!!!!!!!'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch_sample[\"tokenized_first_sentence\"][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Centricity Music released the project on October 14, 2014. Aaron Shust worked with producers James Fitzpatrick and David Hamilton in the creation of this album..'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch_sample[\"tokenized_next_50_tokens\"][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing the forward pass on temp\n",
    "# log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "#     hypernetwork.forward(\n",
    "#         temp[\"tokenized_first_sentence\"],\n",
    "#         temp[\"tokenized_next_50_tokens\"],\n",
    "#         stop_editing_index=stop_editing_index,\n",
    "#         output_target_hidden_states=True,\n",
    "#         output_edited_hidden_states=True,\n",
    "#         output_edit_vectors=True,\n",
    "#     )[\"logits\"][:, :, :].reshape(-1, 50257),\n",
    "#     dim=1,\n",
    "# )\n",
    "# soft_labels = f_data_to_soft_labels(\n",
    "#     temp[\"tokenized_first_sentence\"],\n",
    "#     temp[\"tokenized_next_50_tokens\"],\n",
    "#     num_predictions_max=50\n",
    "# )\n",
    "# mask = (temp[\"tokenized_next_50_tokens\"] != 0).reshape(-1)\n",
    "# torch.where(torch.isnan(soft_labels)[:,0] & mask)\n",
    "# tokenizer.decode(temp[\"tokenized_next_50_tokens\"].reshape(-1)[650:700])\n",
    "# tokenizer.decode(temp[\"tokenized_next_50_tokens\"][12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 0/30864 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY SHAPE (target activations) torch.Size([32, 104, 1536])\n",
      "KEY SHAPE (editor) torch.Size([32, 1536])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "Epoch 1/200:   0%|          | 0/30864 [00:09<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#current problem: 1728/ 30864 \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m hypernetwork\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_loader\u001b[39m=\u001b[39;49mdata_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     test_loader\u001b[39m=\u001b[39;49mtest_data_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     stop_editing_index\u001b[39m=\u001b[39;49mstop_editing_index,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     lam \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m,\u001b[39m#20000\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     lam_testing_penalty \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     KL_divergence_loss\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     f_data_to_soft_labels \u001b[39m=\u001b[39;49m f_data_to_soft_labels,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb Cell 34\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=406'>407</a>\u001b[0m log_prob_predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=407'>408</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction[\u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m][:, :, :]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m50257\u001b[39m),\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=408'>409</a>\u001b[0m     dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=409'>410</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=410'>411</a>\u001b[0m \u001b[39m# Now we must compute the soft labels! This is outsourced to the user-provided function, teacher_model\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=411'>412</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoft_labels \u001b[39m=\u001b[39m f_data_to_soft_labels(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=412'>413</a>\u001b[0m     batch[\u001b[39m\"\u001b[39;49m\u001b[39mtokenized_first_sentence\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=413'>414</a>\u001b[0m     batch[\u001b[39m\"\u001b[39;49m\u001b[39mtokenized_next_50_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=414'>415</a>\u001b[0m     num_predictions_max\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=415'>416</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=416'>417</a>\u001b[0m \u001b[39m# check that the mask makes sense!\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=417'>418</a>\u001b[0m mask \u001b[39m=\u001b[39m (batch[\u001b[39m\"\u001b[39m\u001b[39mtokenized_next_50_tokens\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m50256\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Extract the predictions corresponding to B\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(lengths_A)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     hold_output[i, :lengths_B[i], :] \u001b[39m=\u001b[39m predictions[i, lengths_A[i]:lengths_A[i] \u001b[39m+\u001b[39;49m lengths_B[i], :]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m hold_output\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m50257\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/interp/lib/python3.10/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#current problem: 1728/ 30864 \n",
    "hypernetwork.train(\n",
    "    train_loader=data_loader,\n",
    "    test_loader=test_data_loader,\n",
    "    stop_editing_index=stop_editing_index,\n",
    "    epochs=200,\n",
    "    lam = 0,#20000\n",
    "    lam_testing_penalty = 0,\n",
    "    KL_divergence_loss=True,\n",
    "    f_data_to_soft_labels = f_data_to_soft_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at some outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader:\n",
    "    temp = batch\n",
    "    break\n",
    "hypernetwork.inspect_batch_prediction_ouptuts(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader:\n",
    "    result = hypernetwork.forward(\n",
    "        editor_input_ids=batch[\"tokenized_first_sentence\"],\n",
    "        target_input_ids=batch[\"tokenized_next_50_tokens\"],\n",
    "        stop_editing_index=stop_editing_index,\n",
    "        output_target_hidden_states=True,\n",
    "        output_edited_hidden_states=True,\n",
    "        output_edit_vectors=True,\n",
    "        output_editor_attention=True,\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypernetwork.evaluate_KL_test_loss_nogradient(data_loader, f_data_to_soft_labels, stop_editing_index=stop_editing_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoding the first element of batch\n",
    "tokenizer.decode(batch[\"tokenized_next_50_tokens\"][0]), tokenizer.decode(batch[\"tokenized_next_50_tokens\"][0]), tokenizer.decode(batch[\"tokenized_first_sentence\"][0][0]), tokenizer.decode(batch[\"tokenized_first_sentence\"][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"edit_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output['edited_hidden_states'][0][0,7][0:10] - model(cat_example,output_hidden_states = True).hidden_states[0][0][7][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit_embedding[0:10]\n",
    "result['editor_attention'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block calculates edit norm relative to the size of the current activations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "for batch_index in range(min(20,len(batch[\"tokenized_next_50_tokens\"]))):\n",
    "\n",
    "    #The tensor norm comes in an 8x13 matrix\n",
    "    edit_tensor = result[\"edit_vectors\"][batch_index].to(\"cpu\")\n",
    "    edit_tensor[:8,:, :] = edit_tensor[:8,:,:]/result[\"target_hidden_states\"][batch_index].norm(dim=2, keepdim=True) .to(\"cpu\")\n",
    "    edit_tensor_norm = edit_tensor.norm(dim=2)\n",
    "\n",
    "    # is this any better??\n",
    "    # attention_matrix = result['editor_attention'][batch_index].reshape(104).to(\"cpu\").reshape(13,8).permute(1,0)\n",
    "\n",
    "    # Detach and convert to numpy\n",
    "    edit_tensor_norm = edit_tensor_norm.detach().numpy()[0:stopping_index, :]\n",
    "\n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.imshow(edit_tensor_norm, cmap=\"hot\")\n",
    "\n",
    "    # Color the heatmap according to the entry sizes\n",
    "    heatmap.set_clim(vmin=np.min(0), vmax=np.max(edit_tensor_norm))\n",
    "    cbar = plt.colorbar(heatmap)\n",
    "    cbar.set_label(\"Entry Sizes\")\n",
    "\n",
    "    # Add labels to the x and y axes\n",
    "    ax.set_xticks(np.arange(13))\n",
    "    ax.set_yticks(np.arange(8))\n",
    "    ax.set_xticklabels(np.arange(13))\n",
    "    ax.set_yticklabels(np.arange(8))\n",
    "\n",
    "    # Rotate the x-axis labels\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Add a title\n",
    "    plt.title(\"Edit Norm / Target Norm Heatmap\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    print(tokenizer.batch_decode(batch[\"tokenized_next_50_tokens\"][batch_index][0:8]))\n",
    "    print(tokenizer.batch_decode(batch[\"tokenized_first_sentence\"][batch_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "for batch_index in range(min(10,len(batch[\"tokenized_next_50_tokens\"]))):\n",
    "\n",
    "    #The tensor norm comes in an 8x13 matrix\n",
    "    edit_tensor = result[\"edit_vectors\"][batch_index].to(\"cpu\")\n",
    "    edit_tensor_norm = edit_tensor.norm(dim=2)\n",
    "\n",
    "    # is this any better??\n",
    "    # attention_matrix = result['editor_attention'][batch_index].reshape(104).to(\"cpu\").reshape(13,8).permute(1,0)\n",
    "\n",
    "    # Detach and convert to numpy\n",
    "    edit_tensor_norm = edit_tensor_norm.detach().numpy()[0:stopping_index, :]\n",
    "\n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.imshow(edit_tensor_norm, cmap=\"hot\")\n",
    "\n",
    "    # Color the heatmap according to the entry sizes\n",
    "    heatmap.set_clim(vmin=np.min(0), vmax=np.max(edit_tensor_norm))\n",
    "    cbar = plt.colorbar(heatmap)\n",
    "    cbar.set_label(\"Entry Sizes\")\n",
    "\n",
    "    # Add labels to the x and y axes\n",
    "    ax.set_xticks(np.arange(13))\n",
    "    ax.set_yticks(np.arange(8))\n",
    "    ax.set_xticklabels(np.arange(13))\n",
    "    ax.set_yticklabels(np.arange(8))\n",
    "\n",
    "    # Rotate the x-axis labels\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Add a title\n",
    "    plt.title(\"Edit Norm Heatmap\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    print(tokenizer.batch_decode(batch[\"tokenized_next_50_tokens\"][batch_index][0:8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "stopping_index = 8\n",
    "\n",
    "for head_index in range(min(hypernetwork.editor_model.config.num_editing_heads,10)):\n",
    "    for batch_index in range(3):\n",
    "\n",
    "        # Reshape the tensor into an 8x13 matrix\n",
    "        attention_matrix = result[\"editor_attention\"][batch_index][head_index].reshape(8, 13).to(\"cpu\")\n",
    "\n",
    "        # is this any better??\n",
    "        # attention_matrix = result['editor_attention'][batch_index].reshape(104).to(\"cpu\").reshape(13,8).permute(1,0)\n",
    "\n",
    "        # Detach and convert to numpy\n",
    "        attention_matrix = attention_matrix.detach().numpy()\n",
    "\n",
    "        # Create the heatmap\n",
    "        fig, ax = plt.subplots()\n",
    "        heatmap = ax.imshow(attention_matrix, cmap=\"hot\")\n",
    "\n",
    "        # Color the heatmap according to the entry sizes\n",
    "        heatmap.set_clim(vmin=np.min(attention_matrix), vmax=np.max(attention_matrix))\n",
    "        cbar = plt.colorbar(heatmap)\n",
    "        cbar.set_label(\"Entry Sizes\")\n",
    "\n",
    "        # Add labels to the x and y axes\n",
    "        ax.set_xticks(np.arange(13))\n",
    "        ax.set_yticks(np.arange(8))\n",
    "        ax.set_xticklabels(np.arange(13))\n",
    "        ax.set_yticklabels(np.arange(8))\n",
    "\n",
    "        # Rotate the x-axis labels\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "        # Add a title\n",
    "        plt.title(\"Editor Attention Heatmap\")\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        print(tokenizer.batch_decode(batch[\"result_text\"][batch_index]))\n",
    "        print(tokenizer.batch_decode(batch[\"editor_tokens\"][batch_index][8:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.norm(\n",
    "#     result[\"edit_vectors\"][batch_index][:stopping_index, :, :].to(\"cpu\"), dim=[0, 2]\n",
    "# )  # looks better now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: It seems, currently, like we are not giving sufficient incentive to intervene at the lowest possible layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"edit_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_tensor = result[\"edit_vectors\"][batch_index].reshape(8, 13, -1).to(\"cpu\")\n",
    "edit_tensor_norm = edit_tensor.norm(dim=2)\n",
    "edit_tensor_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_states = torch.stack(result['edited_hidden_states'],dim = 2)\n",
    "edited_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    print(edited_states[0][i][0].norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    torch.norm(result['target_hidden_states'][0][6][0]),\n",
    "    torch.norm(result['target_hidden_states'][0][0][0]),\n",
    "    edited_states[0][6][0].norm(),\n",
    "    edited_states[0][2][0].norm()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(hypernetwork.state_dict(), \"/root/aiplay-1/hypernetworks/hypernetwork.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
