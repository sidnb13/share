{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, run\n",
    "download_wikipedia.ipynb \n",
    "to get the file\n",
    "wikipedia_three_sentences.csv\n",
    "\n",
    "Then, we do our processing here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_index=8 #4090 or A6000\n",
    "num_editing_heads = 768*2 #more seems to be better for this #per sid's suggestion: can add more heads in every layer. This is probably a really great suggestion\n",
    "editor_channel_width = 768 * 2\n",
    "max_grad_clip = 4.0\n",
    "chop_layer = 6\n",
    "lr = 4e-4\n",
    "edit_dampening_factor=1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes from Sid: SAE vs principal components\n",
    "-you could try to work on the SAE's???\n",
    "-get discrete targets\n",
    "[I wouldn't be surprised at all if this helped a lot]\n",
    "\n",
    "\n",
    "-Next deliverables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch['tokenized_first_sentence'][30].size(), batch['tokenized_next_50_tokens'][30].size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichaelbsklar\u001b[0m (\u001b[33mmichaelsklar\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/share/hypernetworks/wandb/run-20240430_044123-craq6rc7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/michaelsklar/hypernetworks/runs/craq6rc7' target=\"_blank\">logical-capybara-302</a></strong> to <a href='https://wandb.ai/michaelsklar/hypernetworks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/michaelsklar/hypernetworks' target=\"_blank\">https://wandb.ai/michaelsklar/hypernetworks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/michaelsklar/hypernetworks/runs/craq6rc7' target=\"_blank\">https://wandb.ai/michaelsklar/hypernetworks/runs/craq6rc7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/michaelsklar/hypernetworks/runs/craq6rc7?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff376488850>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> 74203ae363958b1962be144bcd83036c8e28ea2a
   "source": [
    "# # Trying out the editor hypernetwork on the dune dataset\n",
    "import wandb\n",
    "\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"hypernetworks\",\n",
    "#     config={\"targetmodel\": \"gpt2\", \"editormodel\": \"gpt2\"},\n",
    "# )\n",
    "# # Copy this below where needed!\n",
    "# # run.log_model(path=\"<path-to-model>\", name=\"<name>\")\n",
    "\n",
    "# # wandb.log({\"acc\": acc, \"loss\": loss})\n",
    "\n",
    "# # wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CUDA_LAUNCH_BLOCKING=1\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# Set torch default device\n",
    "import torch\n",
    "# torch.set_default_device(\"cuda\")\n",
    "torch.set_default_device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, GPT2Model\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "import torch\n",
    "from torch import compile\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import contextlib\n",
    "import os\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cross_attention_to_layer(block, config):\n",
    "    block.crossattention = GPT2Attention(config, is_cross_attention=True)\n",
    "    block.ln_cross_attn = nn.LayerNorm(\n",
    "        normalized_shape=768, eps=config.layer_norm_epsilon\n",
    "    )\n",
    "    original_query_weights = block.attn.c_attn.weight[:, :768]\n",
    "    original_keys_values = block.attn.c_attn.weight[:, 768:]\n",
    "    original_query_bias = block.attn.c_attn.bias[:768]\n",
    "    original_keys_values_bias = block.attn.c_attn.bias[768:]\n",
    "    with torch.no_grad():\n",
    "        # Initialize the new layer with these parameters\n",
    "        block.crossattention.q_attn.weight = nn.Parameter(original_query_weights)\n",
    "        block.crossattention.q_attn.bias = nn.Parameter(original_query_bias)\n",
    "        block.crossattention.c_attn.weight = nn.Parameter(original_keys_values)\n",
    "        block.crossattention.c_attn.bias = nn.Parameter(original_keys_values_bias)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we stick on the \"reverse attention\" module at the end!\n",
    "This is a customized attention head that reads from the editor model, and writes to the target model's activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Editor_Attention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Controls whether the head will do a global softmax in all positions & layers\n",
    "        # If True, the attn is global and will sum to 1\n",
    "        # If False, the attn is a logistic fxn independently for every layer & token\n",
    "        # I suspect we will also want to penalize the intervention norm\n",
    "        self.num_editing_heads = (\n",
    "            config.num_editing_heads\n",
    "        )  # should default to 1, but we're going to test adding more\n",
    "        self.edit_channel_width = config.edit_channel_width\n",
    "        if self.edit_channel_width % self.num_editing_heads != 0:\n",
    "            print(\"Error: config hidden size is not divisible by num_editing_heads\")\n",
    "        self.head_dim = self.edit_channel_width // self.num_editing_heads\n",
    "        self.embed_dim = config.hidden_size\n",
    "\n",
    "        max_positions = (\n",
    "            config.max_position_embeddings\n",
    "        )  # does this do anything? can try killing this later\n",
    "        self.register_buffer(\n",
    "            \"bias\",\n",
    "            torch.tril(\n",
    "                torch.ones((max_positions, max_positions), dtype=torch.bool)\n",
    "            ).view(1, 1, max_positions, max_positions),\n",
    "            persistent=False,\n",
    "        )\n",
    "        self.register_buffer(\"masked_bias\", torch.tensor(-1e4), persistent=False)\n",
    "\n",
    "        # We compute Q and K as a single nn.linear; but will later break apart into subcomponents\n",
    "\n",
    "        ## Before modification to a variable channel-width\n",
    "        # self.q_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.k_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.v_attn = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        # self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "\n",
    "        self.q_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.k_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.v_attn = nn.Linear(self.embed_dim, self.edit_channel_width)\n",
    "        self.out_proj = nn.Linear(self.edit_channel_width, self.embed_dim)\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        \"\"\"Split the last dimension into (num_heads, head_dim).\"\"\"\n",
    "        new_shape = x.size()[:-1] + (self.num_editing_heads, self.head_dim)\n",
    "        return x.view(*new_shape)\n",
    "\n",
    "    def _new_reverse_attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        # Assume that we are doing softmax attention\n",
    "\n",
    "        print(\"QUERY SHAPE (target activations)\", query.shape)\n",
    "        print(\"KEY SHAPE (editor)\", key.shape)\n",
    "\n",
    "        # Project and split the query, key, value tensors\n",
    "        split_query = self._split_heads(query)\n",
    "        split_key = self._split_heads(key)\n",
    "        split_value = self._split_heads(value)\n",
    "\n",
    "        # Double-application (is this actually good/better for some reason?)\n",
    "        # self._split_heads(self.q_attn(query))\n",
    "        # self._split_heads(self.k_attn(key))\n",
    "        # self._split_heads(self.v_attn(value))\n",
    "\n",
    "        if split_query.dim() != 4:\n",
    "            print(\"Error: Expected query to be 4D tensor, but got something else!\")\n",
    "        if split_key.dim() != 3:\n",
    "            print(\"Error: Expected key to be 3D tensor, but got something else!\")\n",
    "        if split_value.dim() != 3:\n",
    "            print(\"Error: Expected value to be 3D tensor, but got something else!\")\n",
    "\n",
    "        # Query should be shaped as (batch_index, sequence_index, head_index, head_dim)\n",
    "        # Key and value should be shaped as (batch_index, head_index, head_dim)\n",
    "\n",
    "        KQ_weights = torch.matmul(\n",
    "            split_query.permute(0, 2, 1, 3), split_key.unsqueeze(-1)\n",
    "        ).squeeze(-1)\n",
    "\n",
    "        # Then we take the softmax within the positional divisions\n",
    "        softmaxed_weights = nn.functional.softmax(KQ_weights, dim=-1)\n",
    "\n",
    "        # Adjusting value selection for head dimension\n",
    "        attn_output = torch.matmul(\n",
    "            softmaxed_weights.unsqueeze(-1), split_value.unsqueeze(-2)\n",
    "        )\n",
    "\n",
    "        # combine heads: change 50, 8, 104, 96 to 50, 104, 768\n",
    "        # first, permute\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3)\n",
    "        # combin heads x head_dims\n",
    "        attn_output = attn_output.reshape(\n",
    "            -1, attn_output.size(1), attn_output.size(2) * attn_output.size(3)\n",
    "        )\n",
    "        # now project back\n",
    "        projected_output = self.out_proj(attn_output)\n",
    "\n",
    "        return projected_output, softmaxed_weights\n",
    "\n",
    "    def _reverse_attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        if key.dim() == 4:\n",
    "            K_reduced = key[\n",
    "                :, :, -1, :\n",
    "            ]  # R# Check: that the second dimension of K is only a single element when we have batching\n",
    "            KQ_weights = torch.bmm(K_reduced, query.transpose(1, 2))\n",
    "            logistic_weights = torch.atan(KQ_weights)\n",
    "            attn_output = torch.bmm(\n",
    "                logistic_weights.transpose(1, 2),\n",
    "                value[\n",
    "                    :, :, -1, :\n",
    "                ],  # we take the editor output only over the final token position\n",
    "            )\n",
    "\n",
    "        if key.dim() == 3:\n",
    "            QK_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "            logistic_weights = torch.atan(QK_weights)\n",
    "            attn_output = torch.matmul(logistic_weights, value)\n",
    "\n",
    "        return attn_output, logistic_weights\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        editor_hidden_states,\n",
    "        target_hidden_states,\n",
    "        attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # Here, the query is the target hidden encoder, the key is the editor, and the value is the editor\n",
    "        query = self.q_attn(target_hidden_states)\n",
    "        if editor_hidden_states.dim() == 3:\n",
    "            key = self.k_attn(\n",
    "                # I don't quite understand why sometimes editor_hidden_states is 4 dimensional, sometimes 3\n",
    "                # seems like it's sometimes 20, 1, 4, 768 and sometimes 20, 4, 768. what gives?\n",
    "                editor_hidden_states[:, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "            value = self.v_attn(\n",
    "                # [:, 0, :1, :]\n",
    "                editor_hidden_states[:, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "\n",
    "        if editor_hidden_states.dim() == 4:\n",
    "            key = self.k_attn(\n",
    "                editor_hidden_states[:, 0, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "            value = self.v_attn(\n",
    "                # [:, 0, :1, :]\n",
    "                editor_hidden_states[:, 0, -1, :]\n",
    "            )  # Pull only the final token position\n",
    "\n",
    "        attn_output, attn_weights = self._new_reverse_attn(query, key, value)\n",
    "\n",
    "        if output_attentions:\n",
    "            return attn_output, attn_weights\n",
    "        else:\n",
    "            return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "\n",
    "def new_forward(\n",
    "    self,\n",
    "    input_ids: Optional[torch.LongTensor] = None,\n",
    "    past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
    "    attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    token_type_ids: Optional[torch.LongTensor] = None,\n",
    "    position_ids: Optional[torch.LongTensor] = None,\n",
    "    head_mask: Optional[torch.FloatTensor] = None,\n",
    "    inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "    encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "    encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "    # labels: Optional[torch.LongTensor] = None,\n",
    "    use_cache: Optional[bool] = None,\n",
    "    output_attentions: Optional[bool] = None,\n",
    "    output_hidden_states: Optional[bool] = None,\n",
    "    return_dict: Optional[bool] = None,\n",
    ") -> Union[Tuple]:\n",
    "    r\"\"\"\n",
    "    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "        Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n",
    "        `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n",
    "        are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n",
    "    \"\"\"\n",
    "\n",
    "    transformer_outputs = self.transformer(\n",
    "        input_ids,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=attention_mask,\n",
    "        token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids,\n",
    "        head_mask=head_mask,\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        encoder_hidden_states=encoder_hidden_states,\n",
    "        encoder_attention_mask=encoder_attention_mask,\n",
    "        use_cache=use_cache,\n",
    "        output_attentions=output_attentions,\n",
    "        output_hidden_states=output_hidden_states,\n",
    "        return_dict=return_dict,\n",
    "    )\n",
    "    hidden_states = transformer_outputs[0]\n",
    "\n",
    "    # Set device for model parallelism\n",
    "    if self.model_parallel and torch.cuda.is_available():\n",
    "        torch.cuda.set_device(self.transformer.first_device)\n",
    "        hidden_states = hidden_states.to(self.lm_head.weight.device)\n",
    "\n",
    "    # lm_logits = self.lm_head(hidden_states)\n",
    "    reverse_attention_output = self.lm_head(\n",
    "        hidden_states, encoder_hidden_states, output_attentions=output_attentions\n",
    "    )\n",
    "\n",
    "    return reverse_attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_final_layer_with_bespoke_reverse_attention(model):\n",
    "    model.lm_head = Editor_Attention(config=model.config)\n",
    "    model.forward = new_forward.__get__(model, GPT2LMHeadModel)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def add_fwd_hooks(module_hooks):\n",
    "    \"\"\"\n",
    "    Context manager for temporarily adding forward hooks to a model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    module_hooks\n",
    "        A list of pairs: (module, fnc) The function will be registered as a\n",
    "            forward hook on the module\n",
    "    \"\"\"\n",
    "    try:\n",
    "        handles = []\n",
    "        for mod, hk in module_hooks:\n",
    "            handles.append(mod.register_forward_hook(hk))\n",
    "        yield\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_layer_indices(model):\n",
    "    \"\"\"\n",
    "    Assigns a custom attribute 'layer_index' to each transformer layer in the GPT-2 model.\n",
    "    This function iterates over the transformer blocks and assigns an index to each.\n",
    "    \"\"\"\n",
    "    model.transformer.wte.layer_index = 0\n",
    "    for i, layer in enumerate(model.transformer.h):\n",
    "        layer.layer_index = i + 1\n",
    "\n",
    "\n",
    "# Usage:\n",
    "# assign_layer_indices(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_and_pad(A, B):\n",
    "    batch_size, n_tokens_A = A.size()\n",
    "    n_tokens_B = B.size(1)\n",
    "\n",
    "    #Find the lengths in A and B\n",
    "    lengths_A = torch.sum(A != 50256, dim=1)\n",
    "    lengths_B = torch.sum(B != 50256, dim=1)\n",
    "\n",
    "    #iniitalize empty tensor\n",
    "    result = torch.full((batch_size, max(lengths_A + lengths_B),), 50256)\n",
    "\n",
    "    #Concatenate A[i] and B[i] a\n",
    "    for i in range(batch_size):\n",
    "        result[i, : lengths_A[i]] = A[i, : lengths_A[i]]\n",
    "        result[i, lengths_A[i] : lengths_A[i] + lengths_B[i]] = B[i, : lengths_B[i]]\n",
    "\n",
    "    # # Concatenate A and B along dimension 1\n",
    "    # concatenated = torch.cat((A, B), dim=1)\n",
    "    \n",
    "    # # Find the number of non-zero elements in each row of concatenated tensor\n",
    "    # lengths = torch.sum(concatenated != 50256, dim=1)\n",
    "    \n",
    "    # # Create a mask to identify the non-zero elements in concatenated tensor\n",
    "    # mask = torch.arange(n_tokens_A + n_tokens_B).expand(batch_size, n_tokens_A + n_tokens_B) < lengths.unsqueeze(1)\n",
    "    \n",
    "    # # Create a new tensor with the same shape as concatenated tensor\n",
    "    # result = torch.full_like(concatenated, 50256)\n",
    "    \n",
    "    # # Move the non-zero elements to the left and zero elements to the right\n",
    "    # result[mask] = concatenated[mask]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_nan_gradients(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None and torch.isnan(param.grad).any():\n",
    "            print(f\"NaN values found in gradient of parameter: {name}\")\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 29,
=======
   "execution_count": null,
>>>>>>> 74203ae363958b1962be144bcd83036c8e28ea2a
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.compile #Apparently this fails when used inside jupyter notebooks but is fine if i make dedicated scripts\n",
    "class EditorHypernetwork(nn.Module):\n",
    "    # Separating the editor config file, from its base model's configurations\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_editing_heads=32,\n",
    "        edit_channel_width=768,  # controls dimensionality given to attention heads in the last layer of the editor\n",
    "        use_layerwise_embeddings=True,\n",
    "        chop_editor_at_layer=None,\n",
    "        edit_dampening_factor=0.001,  # tuning parameter to help the edits not be initialized too large\n",
    "        kill_token_zero=False,  # multiplies edits to token pos zero by zero\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Construct Editor Model\n",
    "        # Load the configuration from the YAML file\n",
    "        # with open(editor_yaml_file_path, 'r') as file:\n",
    "        #     self.config = yaml.safe_load(file)\n",
    "        if torch.cuda.is_available():\n",
    "            self.editor_model = (\n",
    "                GPT2LMHeadModel.from_pretrained(\"gpt2\").cuda().eval()\n",
    "            )  # have recently added .cuda() so it uses the gpu\n",
    "        else:\n",
    "            self.editor_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(\"mps\").eval()\n",
    "\n",
    "        # Add cross-attention to each layer\n",
    "        self.editor_model.config.add_cross_attention = True\n",
    "        self.editor_model.config.num_editing_heads = num_editing_heads\n",
    "        self.editor_model.config.chop_layer = chop_editor_at_layer\n",
    "        self.editor_model.config.kill_token_zero = kill_token_zero\n",
    "        self.editor_model.config.edit_channel_width = edit_channel_width\n",
    "\n",
    "        if chop_editor_at_layer is None:\n",
    "            chop_editor_at_layer = 12\n",
    "\n",
    "        for i in range(chop_editor_at_layer):\n",
    "            add_cross_attention_to_layer(\n",
    "                self.editor_model.transformer.h[i], self.editor_model.config\n",
    "            )\n",
    "\n",
    "        # Delete extra layers beyond the chop_layer\n",
    "        self.editor_model.transformer.h = self.editor_model.transformer.h[\n",
    "            :chop_editor_at_layer\n",
    "        ]\n",
    "\n",
    "        # Replace the final linear layer with special reverse attention output\n",
    "        self.editor_model.lm_head = Editor_Attention(config=self.editor_model.config)\n",
    "        self.editor_model.forward = new_forward.__get__(\n",
    "            self.editor_model, GPT2LMHeadModel\n",
    "        )\n",
    "        if torch.cuda.is_available():\n",
    "            self.editor_model.cuda()\n",
    "        else:\n",
    "            self.editor_model.to(\"mps\")\n",
    "\n",
    "        # Construct Target Model\n",
    "        if torch.cuda.is_available():\n",
    "            self.target_model = (\n",
    "                transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\").cuda().eval()\n",
    "            )\n",
    "        else:\n",
    "            self.target_model = (\n",
    "                transformers.AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "                .to(\"mps\")\n",
    "                .eval()\n",
    "            )\n",
    "        for param in self.target_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        assign_layer_indices(self.target_model)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            self.target_model.cuda()\n",
    "        else:\n",
    "            self.target_model.to(\"mps\")\n",
    "\n",
    "        # Add module for layerwise embeddings\n",
    "        if use_layerwise_embeddings:\n",
    "            self.use_layerwise_embeddings = True\n",
    "            self.layerwise_embeddings = torch.randn(13, 768, requires_grad=True).to(\n",
    "                \"cuda\" if torch.cuda.is_available() else \"mps\"\n",
    "            )\n",
    "        else:\n",
    "            self.use_layerwise_embeddings = False\n",
    "            self.layerwise_embeddings = 0\n",
    "\n",
    "        self.edit_dampening_factor = edit_dampening_factor\n",
    "\n",
    "        self.residual_cache = None\n",
    "        self.opt = None\n",
    "        self.lossfn = None\n",
    "        self.lam = None\n",
    "        self.penalty_loss = None\n",
    "        self.training_loss = None\n",
    "\n",
    "    # Gets the hidden states from the target model, if necessary\n",
    "    def run_target_model_for_encoded_hidden_states(self, target_ids):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.target_model(target_ids, output_hidden_states=True)\n",
    "            hidden_states = outputs.hidden_states\n",
    "            return hidden_states\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        editor_input_ids,\n",
    "        target_input_ids,\n",
    "        target_hidden_states=None,\n",
    "        output_target_hidden_states=False,\n",
    "        output_edited_hidden_states=False,\n",
    "        output_edit_vectors=False,\n",
    "        output_editor_attention=False,\n",
    "        stop_editing_index=None,\n",
    "        batch_edit_vectors=None,\n",
    "    ):\n",
    "        # Run target model for encoded hidden states\n",
    "        if target_hidden_states is None:\n",
    "            target_hidden_states = torch.stack(\n",
    "                self.run_target_model_for_encoded_hidden_states(\n",
    "                    target_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\")\n",
    "                ),  # seems to break while we are passing thru batch_size=1; the last (12th =) has different dimensions\n",
    "                dim=2,\n",
    "            )\n",
    "        # dimensions of target_hidden_states:\n",
    "        # batch_size, token_sequence_length, num_layers = 13, resid_width = 768\n",
    "\n",
    "        # If we are stopping editing at stop_editing_index, then we eliminate target_hidden_states beyond that index\n",
    "        if stop_editing_index is not None:\n",
    "            target_hidden_states = target_hidden_states[\n",
    "                :, :stop_editing_index, :, :\n",
    "            ].clone()\n",
    "\n",
    "        # Normalize along the last dimension\n",
    "        normalization_factors = target_hidden_states.norm(dim=-1, keepdim=True)\n",
    "        target_hidden_states = target_hidden_states / normalization_factors\n",
    "\n",
    "        # Error catching:\n",
    "        if batch_edit_vectors is not None:\n",
    "            if output_edit_vectors or output_editor_attention:\n",
    "                return \"Error: Inputting your own batch_edit_vectors means the model does not construct the outputs you are requesting\"\n",
    "\n",
    "        # Run editor model, get edit vectors\n",
    "        if batch_edit_vectors is None:\n",
    "            if self.use_layerwise_embeddings:\n",
    "                # Now, add in the layerwise embeddings\n",
    "                embedded_hidden_states = (\n",
    "                    target_hidden_states + self.layerwise_embeddings[None, None, :, :]\n",
    "                )\n",
    "\n",
    "                collapsed_target_hidden_states = embedded_hidden_states.reshape(\n",
    "                    target_hidden_states.shape[0],\n",
    "                    target_hidden_states.shape[1] * target_hidden_states.shape[2],\n",
    "                    target_hidden_states.shape[3],\n",
    "                )\n",
    "            else:\n",
    "                collapsed_target_hidden_states = target_hidden_states.reshape(\n",
    "                    target_hidden_states.shape[0],\n",
    "                    target_hidden_states.shape[1] * target_hidden_states.shape[2],\n",
    "                    target_hidden_states.shape[3],\n",
    "                )\n",
    "\n",
    "            editor_output = self.editor_model(\n",
    "                editor_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\"),\n",
    "                encoder_hidden_states=collapsed_target_hidden_states,\n",
    "                output_attentions=output_editor_attention,\n",
    "            )\n",
    "            # Multiply the outputs by normalization factors\n",
    "            if output_editor_attention:\n",
    "                temp_edit_vectors = editor_output[0]\n",
    "                # Might want to reshape this too but whatever\n",
    "                batch_editor_attention = editor_output[1]\n",
    "            else:\n",
    "                temp_edit_vectors = editor_output\n",
    "\n",
    "            # Renormalize to the scale of the target hidden states\n",
    "            # and reshape to proper dimensions\n",
    "            batch_edit_vectors = (\n",
    "                self.edit_dampening_factor\n",
    "                * normalization_factors\n",
    "                * temp_edit_vectors.reshape(\n",
    "                    temp_edit_vectors.shape[0], stop_editing_index, 13, 768\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # If we are stopping editing at stop_editing_index,\n",
    "        # this pads batch_edit_vectors with 0's to the right of the edited positions\n",
    "        if stop_editing_index is not None:\n",
    "            batch_edit_vectors = torch.cat(\n",
    "                (\n",
    "                    batch_edit_vectors,\n",
    "                    torch.zeros(\n",
    "                        batch_edit_vectors.shape[0],\n",
    "                        target_input_ids.shape[1] - stop_editing_index,\n",
    "                        13,\n",
    "                        768,\n",
    "                    ),\n",
    "                ),\n",
    "                dim=1,\n",
    "            )\n",
    "\n",
    "        # Run target model with edit vectors. This adds the edit vectors to the given hidden state at the specified batch index, position, and layer\n",
    "        def edit_add(module, input, output):\n",
    "            layer_index = module.layer_index\n",
    "            output[0][:] = output[0] + batch_edit_vectors[:, :, layer_index, :]\n",
    "            if self.editor_model.config.kill_token_zero == True:\n",
    "                output[0][:, 0, :] = 0\n",
    "\n",
    "        def embedding_edit_add(module, input, output):\n",
    "            output[:] = output + batch_edit_vectors[:, :, 0, :]\n",
    "            if self.editor_model.config.kill_token_zero == True:\n",
    "                output[:, 0, :] = 0\n",
    "\n",
    "        # Now editing the target model\n",
    "        hooks1 = [(self.target_model.transformer.wte, embedding_edit_add)]\n",
    "        hooks2 = [(self.target_model.transformer.h[L], edit_add) for L in range(12)]\n",
    "        hooks = hooks1 + hooks2\n",
    "        with add_fwd_hooks(hooks):\n",
    "            # THIS IS THE LINE WHERE THE MODEL IS CALLED (AND THE EDITOR IS CALLED AT\n",
    "            # THE END OF `layer` AS A SIDE EFFECT)\n",
    "            target_result = self.target_model(\n",
    "                target_input_ids.to(\"cuda\" if torch.cuda.is_available() else \"mps\"),\n",
    "                output_hidden_states=output_edited_hidden_states,\n",
    "            )\n",
    "\n",
    "        logits = target_result.logits\n",
    "\n",
    "        output = {}\n",
    "        output[\"logits\"] = logits\n",
    "        if output_target_hidden_states:\n",
    "            output[\"target_hidden_states\"] = (\n",
    "                target_hidden_states * normalization_factors\n",
    "            )\n",
    "        if output_edited_hidden_states:\n",
    "            output[\"edited_hidden_states\"] = target_result.hidden_states\n",
    "        if output_edit_vectors:\n",
    "            output[\"edit_vectors\"] = batch_edit_vectors\n",
    "        if output_editor_attention:\n",
    "            output[\"editor_attention\"] = batch_editor_attention\n",
    "        return output\n",
    "\n",
    "    # Generate text using the target model, with a new edit application at every step.\n",
    "    # This is a very slow way to generate text.\n",
    "    # If you only want to edit first k tokens, use the forward pass instead with stop_editing_index = k\n",
    "    def inspect_batch_prediction_ouptuts(self, batch):\n",
    "        with torch.no_grad():\n",
    "            batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "            self.editor_inputs = batch[\"tokenized_first_sentence\"][i].unsqueeze(0)\n",
    "            self.target_inputs = batch[\"tokenized_next_50_tokens\"][i].unsqueeze(0)\n",
    "            self.prediction = self.forward(\n",
    "                self.editor_inputs,\n",
    "                self.target_inputs,\n",
    "                stop_editing_index=stop_editing_index,\n",
    "                output_target_hidden_states=False,\n",
    "                output_edited_hidden_states=False,\n",
    "                output_edit_vectors=False,\n",
    "                output_editor_attention=False,\n",
    "            )\n",
<<<<<<< HEAD
    "            # compute most likely tokens from the logits\n",
    "            predicted_ids = [\n",
    "                torch.argmax(pred[\"logits\"], dim=-1) for pred in predictions\n",
    "            ]\n",
    "            # convert the token ids to strings\n",
=======
    "            #compute most likely tokens from the logits\n",
    "            predicted_ids = [torch.argmax(pred, dim=-1) for pred in self.prediction[\"logits\"]]\n",
    "            #convert the token ids to strings\n",
>>>>>>> 74203ae363958b1962be144bcd83036c8e28ea2a
    "            predicted_strings = [tokenizer.decode(pred) for pred in predicted_ids]\n",
    "            return predicted_strings\n",
    "\n",
    "    def evaluate_KL_test_loss_nogradient(\n",
    "        self, dataloader, f_data_to_soft_labels=None, stop_editing_index=8\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            sum_weighted_losses = 0.0\n",
    "            total_samples = 0\n",
    "            for batch in dataloader:\n",
    "                current_batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "                self.editor_inputs = batch[\"tokenized_first_sentence\"].squeeze(1)\n",
    "                self.target_inputs = batch[\"tokenized_next_50_tokens\"].squeeze(1)\n",
    "                self.prediction = self.forward(  # check the batch size\n",
    "                    self.editor_inputs,\n",
    "                    self.target_inputs,\n",
    "                    stop_editing_index=stop_editing_index,\n",
    "                )\n",
    "                log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                    self.prediction[\"logits\"][:, stop_editing_index:, :].reshape(\n",
    "                        -1, 50257\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "                # Now we must compute the soft labels!\n",
    "                # join the last 50 tokens to the editor inputs\n",
    "                soft_labels = f_data_to_soft_labels(\n",
    "                    batch[\"tokenized_first_sentence\"],\n",
    "                    batch[\"tokenized_next_50_tokens\"],\n",
    "                    num_predictions_max=50,\n",
    "                )\n",
    "                mask = (batch[\"tokenized_next_50_tokens\"] != 50256).reshape(-1)\n",
<<<<<<< HEAD
    "                self.loss = torch.nn.functional.kl_div(\n",
    "                    log_prob_predictions, soft_labels, reduction=\"batchmean\"\n",
    "                )\n",
=======
    "                self.loss = torch.nn.functional.kl_div(log_prob_predictions[mask,:], soft_labels[mask,:], reduction = 'batchmean')\n",
>>>>>>> 74203ae363958b1962be144bcd83036c8e28ea2a
    "                # Weight the loss by current batch size and update the sum of weighted losses\n",
    "                sum_weighted_losses += self.loss.item() * current_batch_size\n",
    "                total_samples += current_batch_size\n",
    "            weighted_average_loss = sum_weighted_losses / total_samples\n",
    "        return weighted_average_loss\n",
    "\n",
    "    def evaluate_crossentropy_test_loss_nogradient(\n",
    "        self, dataloader, stop_editing_index=8\n",
    "    ):\n",
    "        with torch.no_grad():\n",
    "            sum_weighted_losses = 0.0\n",
    "            total_samples = 0\n",
    "            for batch in dataloader:\n",
    "                # current_batch_size = len(batch[\"tokenized_first_sentence\"])\n",
    "                self.editor_inputs = batch[\"tokenized_first_sentence\"].squeeze(1)\n",
    "                self.target_inputs = batch[\"tokenized_next_50_tokens\"].squeeze(1)\n",
    "                self.prediction = self.forward(  # check the batch size\n",
    "                    self.editor_inputs,\n",
    "                    self.target_inputs,\n",
    "                    stop_editing_index=stop_editing_index,\n",
    "                )\n",
    "                log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                    self.prediction[\"logits\"][:, stop_editing_index:, :].reshape(\n",
    "                        -1, 50257\n",
    "                    ),\n",
    "                    dim=1,\n",
    "                )\n",
    "\n",
    "                # Create a mask to exclude padded tokens\n",
    "                target_labels = self.target_inputs[:, stop_editing_index:].reshape(-1)\n",
    "                mask = (\n",
    "                    target_labels != 50256\n",
    "                )  # Assuming padded tokens are represented by 0\n",
    "\n",
    "                # Compute the cross-entropy loss with masking\n",
    "                criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                loss = criterion(log_prob_predictions, target_labels)\n",
    "                current_mask_sum = mask.sum()\n",
    "                loss = (loss * mask).sum() / current_mask_sum\n",
    "\n",
    "                # Weight the loss by current batch size and update the sum of weighted losses\n",
    "                sum_weighted_losses += loss * current_mask_sum\n",
    "                total_tokens += curr_mask_sum\n",
    "            weighted_average_loss = sum_weighted_losses / total_tokens\n",
    "        return weighted_average_loss\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        train_loader,\n",
    "        test_loader=None,\n",
    "        stop_editing_index=8,\n",
    "        epochs=1,\n",
    "        KL_divergence_loss=False,\n",
    "        lam=0,  # 20000\n",
    "        lam_testing_penalty=0,  # 100000\n",
    "        f_data_to_soft_labels=None,\n",
    "        checkpoint_interval=60,  # save checkpoint every 60 minutes\n",
    "    ):\n",
    "        self.opt = optim.AdamW(\n",
    "            self.parameters(), lr=lr, weight_decay=0.01\n",
    "        )  # usually: lr = 5e-5. 1e-3 worked well!\n",
    "\n",
    "        if KL_divergence_loss:\n",
    "            self.lossfn = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "        else:\n",
    "            self.lossfn = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Create a tqdm progress bar\n",
    "            with tqdm(\n",
    "                total=len(train_loader),\n",
    "                desc=f\"Epoch {epoch + 1}/{epochs}\",\n",
    "                unit=\"batch\",\n",
    "            ) as pbar:\n",
    "                num_datapoints_in_epoch = 0\n",
    "                epoch_train_loss = 0\n",
    "                epoch_gradient_norm = 0\n",
    "                # Train loop\n",
    "                batch_index = -1  # index of first batch will be 0\n",
    "\n",
    "                for batch in (\n",
    "                    train_loader\n",
    "                ):  # not sure what this does for fractional batches. meh whatev\n",
    "                    batch_index += 1\n",
    "                    self.batch = batch\n",
    "                    current_batch_size = len(batch[\"tokenized_next_50_tokens\"])\n",
    "                    num_datapoints_in_epoch += current_batch_size\n",
    "                    self.opt.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    self.prediction = self.forward(\n",
    "                        batch[\"tokenized_first_sentence\"],\n",
    "                        batch[\"tokenized_next_50_tokens\"],\n",
    "                        stop_editing_index=stop_editing_index,\n",
    "                        output_target_hidden_states=True,\n",
    "                        output_edited_hidden_states=True,\n",
    "                        output_edit_vectors=True,\n",
    "                    )\n",
    "\n",
    "                    # Compute the penalty (edit size relative to the hidden state)\n",
    "                    self.lam = lam\n",
    "                    edit_ratio = self.prediction[\"edit_vectors\"].norm(dim=-1)[\n",
    "                        :, :stop_editing_index, :\n",
    "                    ] / self.prediction[\"target_hidden_states\"].norm(dim=-1)\n",
    "                    self.per_datapoint_penalty_loss = self.lam * torch.sum(\n",
    "                        edit_ratio, dim=[1, 2]\n",
    "                    )\n",
    "                    self.penalty_loss = torch.mean(self.per_datapoint_penalty_loss)\n",
    "\n",
    "                    # Compute the data loss\n",
    "                    if KL_divergence_loss:\n",
    "                        log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                            self.prediction[\"logits\"][:, :, :].reshape(-1, 50257),\n",
    "                            dim=1,\n",
    "                        )\n",
    "                        # Now we must compute the soft labels! This is outsourced to the user-provided function, teacher_model\n",
    "                        self.soft_labels = f_data_to_soft_labels(\n",
    "                            batch[\"tokenized_first_sentence\"],\n",
    "                            batch[\"tokenized_next_50_tokens\"],\n",
    "                            num_predictions_max=50,\n",
    "                        )\n",
    "                        # check that the mask makes sense!\n",
    "                        mask = (batch[\"tokenized_next_50_tokens\"] != 50256).reshape(-1)\n",
    "                        self.prediction_loss = self.lossfn(\n",
    "                            log_prob_predictions[mask, :], self.soft_labels[mask, :]\n",
    "                        )\n",
    "                        # NOTE: currently I am letting the loss predict on tokens inside the editing window\n",
    "                        # I didn't do this in the previous testing! Nor is it the case in crossentropy\n",
    "\n",
    "                    else:\n",
    "                        log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "                            self.prediction[\"logits\"][\n",
    "                                :, stop_editing_index:, :\n",
    "                            ].reshape(-1, 50257),\n",
    "                            dim=1,\n",
    "                        )\n",
    "                        # Create a mask to exclude padded tokens\n",
    "                        # NOTE: here we are disallowing prediction on the first stop_editing_index tokens.\n",
    "                        # Code is currently formatted such that this is not the case for KL\n",
    "                        target_labels = batch[\"tokenized_next_50_tokens\"][\n",
    "                            :, stop_editing_index:\n",
    "                        ].reshape(-1)\n",
    "                        mask = target_labels != 50256\n",
    "                        # Compute the cross-entropy loss with masking\n",
    "                        criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "                        loss = criterion(log_prob_predictions, target_labels.long())\n",
    "                        current_mask_sum = mask.sum()\n",
    "                        self.prediction_loss = (loss * mask).sum() / current_mask_sum\n",
    "\n",
    "                    # Compute the total loss and backpropagate\n",
    "                    self.training_loss = self.prediction_loss + self.penalty_loss\n",
    "                    self.training_loss.backward()\n",
    "                    nn.utils.clip_grad_norm_(\n",
    "                        self.parameters(), max_grad_clip\n",
    "                    )  # just implemented this! dunno if a cap of 1 to large, so I'm messing with reducing it\n",
    "\n",
    "                    # Check for nan gradients\n",
    "                    # if check_nan_gradients(self):\n",
    "                    #     break\n",
    "\n",
    "                    # Backwards step\n",
    "                    self.opt.step()\n",
    "\n",
    "                    # metrics\n",
    "                    epoch_train_loss += self.training_loss.item() * current_batch_size\n",
    "                    gradients = [\n",
    "                        p.grad.view(-1) for p in self.parameters() if p.grad is not None\n",
    "                    ]\n",
    "                    all_gradients = torch.cat(gradients)\n",
    "                    gradient_norm = torch.norm(all_gradients).item()\n",
    "                    epoch_gradient_norm += gradient_norm * current_batch_size\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"train_batch_total_loss\": self.training_loss.item(),\n",
    "                            \"train_batch_prediction_loss\": self.prediction_loss.item(),\n",
    "                            \"train_batch_penalty_loss\": self.penalty_loss,\n",
    "                            \"train_batch_gradient_norm\": gradient_norm,\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.update(1)  # note: this was incorrectly displaying before!\n",
    "\n",
    "                    # Check if it's time to save a checkpoint\n",
    "                    current_time = time.time()\n",
    "                    # first loop initialization\n",
    "                    if batch_index == 0 and epoch == 0:\n",
    "                        last_checkpoint_time = -100000\n",
    "\n",
    "                    if current_time - last_checkpoint_time >= checkpoint_interval * 60:\n",
    "                        # Save the checkpoint\n",
    "                        torch.save(\n",
    "                            self.state_dict(),\n",
    "                            f\"checkpoint_epoch_{epoch}_batch_{pbar.n}.pt\",\n",
    "                        )\n",
    "                        last_checkpoint_time = current_time\n",
    "                        # announce checkpoint save\n",
    "                        print(\"Checkpoint saved at epoch\", epoch, \"batch\", pbar.n)\n",
    "\n",
    "                ####END BATCH LOOP\n",
    "                #########################\n",
    "\n",
    "                # epoch loss\n",
    "                # epoch_test_prediction_loss = self.evaluate_crossentropy_test_loss_nogradient(\n",
    "                #     test_loader,\n",
    "                #     stop_editing_index,\n",
    "                #     batch_size\n",
    "                # )\n",
    "                if KL_divergence_loss:\n",
    "                    epoch_test_prediction_loss = self.evaluate_KL_test_loss_nogradient(\n",
    "                        dataloader=test_loader,\n",
    "                        f_data_to_soft_labels=f_data_to_soft_labels,\n",
    "                        stop_editing_index=stop_editing_index,\n",
    "                    )\n",
    "\n",
    "                # # Calculate and accumulate gradient norm for logging\n",
    "                # gradients = [p.grad.view(-1) for p in self.parameters() if p.grad is not None]\n",
    "                # all_gradients = torch.cat(gradients)\n",
    "                # gradient_norm = torch.norm(all_gradients).item()\n",
    "                # epoch_gradient_norm += gradient_norm\n",
    "\n",
    "                if wandb.run:\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"epoch_train_total_loss\": epoch_train_loss\n",
    "                            / num_datapoints_in_epoch,\n",
    "                            \"test_prediction_loss\": epoch_test_prediction_loss,\n",
    "                            \"gradient_norm\": epoch_gradient_norm\n",
    "                            / num_datapoints_in_epoch,\n",
    "                        }\n",
    "                    )\n",
    "            # Save the final model\n",
<<<<<<< HEAD
    "            torch.save(self.state_dict(), \"final_model.pt\")"
=======
    "            torch.save(self, \"final_model.pt\") "
>>>>>>> 74203ae363958b1962be144bcd83036c8e28ea2a
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 30,
=======
   "execution_count": null,
>>>>>>> 74203ae363958b1962be144bcd83036c8e28ea2a
   "metadata": {},
   "outputs": [],
   "source": [
    "#####DIVISION!\n",
    "#DATASET CONSTRUCTION BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = transformers.GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 31,
=======
   "execution_count": null,
>>>>>>> 74203ae363958b1962be144bcd83036c8e28ea2a
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Read the CSV file into a DataFrame\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import os\n",
    "from datasets import Dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "csv_dir = \"wikipedia_three_sentences.csv\"\n",
    "dataframe_dir = \"wikipedia_df.pt\"\n",
    "\n",
    "\n",
    "def tokenize_followup(tokenizer, row):\n",
    "    # Compose second_sentences and third_sentences\n",
    "    followup_text = row[\"second_sentences\"] + \" \" + row[\"third_sentences\"]\n",
    "\n",
    "    # Select the first 500 characters\n",
    "    followup_text = followup_text[:500]\n",
    "\n",
    "    # Tokenize the followup text\n",
    "    tokenized_followup = tokenizer.encode(followup_text)\n",
    "\n",
    "    # Check if the resulting tokenized list is still less than 50 tokens\n",
    "    if len(tokenized_followup) < 50:\n",
    "        # Pad with token 50256 to reach a length of 50 tokens\n",
    "        tokenized_followup = tokenized_followup + [50256] * (\n",
    "            50 - len(tokenized_followup)\n",
    "        )\n",
    "\n",
    "    return tokenized_followup\n",
    "\n",
    "\n",
    "def create_dataframe_from_csv(csv_dir):\n",
    "    df = pd.read_csv(csv_dir)\n",
    "\n",
    "    # # Printng out examples of longest and shortest entries\n",
    "    # # Iterate over each column\n",
    "    # for column in df.columns:\n",
    "    #     print(f\"Column: {column}\")\n",
    "\n",
    "    #     # Find the 10 longest entries in the column\n",
    "    #     longest_entries = df[column].astype(str).apply(len).nlargest(10)\n",
    "    #     print(\"Longest entries:\")\n",
    "    #     for index, length in longest_entries.items():\n",
    "    #         print(f\"Length: {length}, Entry: {df.loc[index, column]}\")\n",
    "\n",
    "    #     # Find the 10 shortest entries in the column\n",
    "    #     shortest_entries = df[column].astype(str).apply(len).nsmallest(10)\n",
    "    #     print(\"Shortest entries:\")\n",
    "    #     for index, length in shortest_entries.items():\n",
    "    #         print(f\"Length: {length}, Entry: {df.loc[index, column]}\")\n",
    "\n",
    "    #     print()\n",
    "\n",
    "    # Length of the dataframe to start\n",
    "    # print(f\"Length of dataframe: {len(df)}\")\n",
    "    # df.columns\n",
    "    # Filter the dataset based on sentence length\n",
    "\n",
    "    df[\"first_sentence_length\"] = df[\"first_sentences\"].apply(lambda x: len(x))\n",
    "    df[\"second_sentence_length\"] = df[\"second_sentences\"].apply(lambda x: len(x))\n",
    "    df[\"third_sentence_length\"] = df[\"third_sentences\"].apply(lambda x: len(x))\n",
    "\n",
    "    df_filtered = df[\n",
    "        (df[\"first_sentence_length\"] >= 5)\n",
    "        & (df[\"second_sentence_length\"] >= 10)\n",
    "        & (df[\"first_sentence_length\"] <= 100)\n",
    "    ]\n",
    "    # print(f\"Length of filtered dataframe: {len(df_filtered)}\")\n",
    "\n",
    "    from transformers import GPT2Tokenizer\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    df_filtered[\"tokenized_first_sentence\"] = df_filtered.apply(\n",
    "        lambda row: tokenizer.encode(row[\"first_sentences\"], add_special_tokens=False),\n",
    "        axis=1,\n",
    "    )\n",
    "    df_filtered[\"tokenized_next_50_tokens\"] = df_filtered.apply(\n",
    "        lambda row: partial(tokenize_followup, tokenizer)(row), axis=1\n",
    "    )\n",
    "    return df_filtered\n",
    "\n",
    "\n",
    "if os.path.exists(\"wikipedia_df.pt\"):\n",
    "    df = pd.read_pickle(\"wikipedia_df.pt\")\n",
    "else:  # save dataset\n",
    "    df = create_dataframe_from_csv(csv_dir)\n",
    "    df.to_pickle(\"wikipedia_df.pt\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 32,
=======
   "execution_count": null,
>>>>>>> 74203ae363958b1962be144bcd83036c8e28ea2a
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    res = {}\n",
    "    for k in batch[0].keys():\n",
    "        if k != '__index_level_0__':\n",
    "            els = [x[k] for x in batch]\n",
    "            max_length = max(len(x) for x in els)\n",
    "            if k == 'tokenized_next_50_tokens':\n",
    "                max_length = 50\n",
    "                for i in range(len(els)):\n",
    "                    if len(els[i]) > 50:\n",
    "                        els[i] = els[i][:50]\n",
    "            # for x in els:\n",
    "            #     x += [0] * (max_length - len(x))\n",
    "            #res[k] = torch.cat([y , torch.zeros(max_length - len(x))],for y in els)\n",
    "            #res[k]=torch.tensor([x + [0] * (max_length( - len(x)) for x in els], dtype=torch.long)\n",
    "            res[k]=torch.stack([ torch.cat((torch.tensor(x),torch.full((max_length - len(x),),50256))) for x in els]).int()\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 33,
=======
   "execution_count": null,
>>>>>>> 74203ae363958b1962be144bcd83036c8e28ea2a
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokenized_first_sentence', 'tokenized_next_50_tokens', '__index_level_0__'],\n",
       "        num_rows: 987633\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokenized_first_sentence', 'tokenized_next_50_tokens', '__index_level_0__'],\n",
       "        num_rows: 85882\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert 2 columns to Dataset and train-test split\n",
    "dataset = Dataset.from_pandas(df[['tokenized_first_sentence', 'tokenized_next_50_tokens']]).shuffle(seed=42)\n",
    "test_ratio = .08\n",
    "temp = dataset.train_test_split(test_size=test_ratio)\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 34,
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> 74203ae363958b1962be144bcd83036c8e28ea2a
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32 #50 or so\n",
    "data_loader = DataLoader(temp[\"train\"], batch_size=batch_size, collate_fn = collate_fn)#batch_size, collate_fn=collate_fn)\n",
    "test_data_loader = DataLoader(temp[\"test\"], batch_size=batch_size, collate_fn = collate_fn)#batch_size, collate_fn=collate_fn)\n",
    "\n",
    "# # Now you can iterate over data_loader in your training loop\n",
    "# j=0\n",
    "# for batch in data_loader:\n",
    "#     j=j+1\n",
    "# print(j)\n",
    "# #Good! we have all the data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['tokenized_first_sentence', 'tokenized_next_50_tokens', '__index_level_0__'],\n",
       "     num_rows: 1073515\n",
       " }),\n",
       " 32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset, data_loader.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
<<<<<<< HEAD
       "<torch.utils.data.dataloader.DataLoader at 0x2e6ecf0d0>"
=======
       "<torch.utils.data.dataloader.DataLoader at 0x7f6c8a3f69e0>"
>>>>>>> 74203ae363958b1962be144bcd83036c8e28ea2a
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in data_loader:\n",
    "#     temp = batch\n",
    "#     break\n",
    "# temp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in data_loader:\n",
    "#     temp = batch\n",
    "#     break\n",
    "# # temp[\"tokenized_next_50_tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_data_to_soft_labels(A, B, num_predictions_max = 50):\n",
    "    with torch.no_grad():\n",
    "        # Compute number of nonzero elements in each row of A and B\n",
    "        lengths_A = torch.sum(A != 50256, dim=1)\n",
    "        lengths_B = torch.sum(B != 50256, dim=1)\n",
    "        # Concatenate A and B along dimension 1\n",
    "        data = concat_and_pad(A, B) \n",
    "        logits = model(data).logits\n",
    "        predictions = torch.nn.functional.softmax(logits, dim=2)\n",
    "\n",
    "        #Create an empty tensor to store the predictions\n",
    "        shape=( len(lengths_A), num_predictions_max, 50257)\n",
    "        hold_output = torch.full(shape, torch.nan)\n",
    "\n",
    "        # Extract the predictions corresponding to B\n",
    "        for i in range(len(lengths_A)):\n",
    "            hold_output[i, :lengths_B[i], :] = predictions[i, lengths_A[i]:lengths_A[i] + lengths_B[i], :]\n",
    "\n",
    "        return hold_output.reshape(-1, 50257) #returns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_editing_index = 8\n",
    "hypernetwork = EditorHypernetwork(\n",
    "    edit_dampening_factor = edit_dampening_factor, #1/10000, \n",
    "    use_layerwise_embeddings=False,\n",
    "    num_editing_heads=num_editing_heads,\n",
    "    edit_channel_width=editor_channel_width,\n",
    "    chop_editor_at_layer=chop_layer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "for batch in data_loader:\n",
    "    if i == 18:\n",
    "        temp = batch\n",
    "        break\n",
    "    i += 1\n",
    "temp[\"tokenized_next_50_tokens\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sample = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['tokenized_first_sentence', 'tokenized_next_50_tokens'])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Unto Us is the sixth studio album by Aaron Shust.!!!!!!!!!!!!!!!'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch_sample[\"tokenized_first_sentence\"][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Centricity Music released the project on October 14, 2014. Aaron Shust worked with producers James Fitzpatrick and David Hamilton in the creation of this album..'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(batch_sample[\"tokenized_next_50_tokens\"][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #testing the forward pass on temp\n",
    "# log_prob_predictions = torch.nn.functional.log_softmax(\n",
    "#     hypernetwork.forward(\n",
    "#         temp[\"tokenized_first_sentence\"],\n",
    "#         temp[\"tokenized_next_50_tokens\"],\n",
    "#         stop_editing_index=stop_editing_index,\n",
    "#         output_target_hidden_states=True,\n",
    "#         output_edited_hidden_states=True,\n",
    "#         output_edit_vectors=True,\n",
    "#     )[\"logits\"][:, :, :].reshape(-1, 50257),\n",
    "#     dim=1,\n",
    "# )\n",
    "# soft_labels = f_data_to_soft_labels(\n",
    "#     temp[\"tokenized_first_sentence\"],\n",
    "#     temp[\"tokenized_next_50_tokens\"],\n",
    "#     num_predictions_max=50\n",
    "# )\n",
    "# mask = (temp[\"tokenized_next_50_tokens\"] != 0).reshape(-1)\n",
    "# torch.where(torch.isnan(soft_labels)[:,0] & mask)\n",
    "# tokenizer.decode(temp[\"tokenized_next_50_tokens\"].reshape(-1)[650:700])\n",
    "# tokenizer.decode(temp[\"tokenized_next_50_tokens\"][12])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 0/30864 [00:00<?, ?batch/s]"
     ]
    },
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY SHAPE (target activations) torch.Size([32, 104, 1536])\n",
      "KEY SHAPE (editor) torch.Size([32, 1536])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "You may ignore this warning if your `pad_token_id` (50256) is identical to the `bos_token_id` (50256), `eos_token_id` (50256), or the `sep_token_id` (None), and your input is not padded.\n",
      "Epoch 1/200:   0%|          | 0/30864 [00:09<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb Cell 34\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m#current problem: 1728/ 30864 \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m hypernetwork\u001b[39m.\u001b[39;49mtrain(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_loader\u001b[39m=\u001b[39;49mdata_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     test_loader\u001b[39m=\u001b[39;49mtest_data_loader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     stop_editing_index\u001b[39m=\u001b[39;49mstop_editing_index,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     lam \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m,\u001b[39m#20000\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     lam_testing_penalty \u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     KL_divergence_loss\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     f_data_to_soft_labels \u001b[39m=\u001b[39;49m f_data_to_soft_labels,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m )\n",
      "\u001b[1;32m/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb Cell 34\u001b[0m line \u001b[0;36m4\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=406'>407</a>\u001b[0m log_prob_predictions \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mlog_softmax(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=407'>408</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprediction[\u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m][:, :, :]\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m50257\u001b[39m),\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=408'>409</a>\u001b[0m     dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=409'>410</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=410'>411</a>\u001b[0m \u001b[39m# Now we must compute the soft labels! This is outsourced to the user-provided function, teacher_model\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=411'>412</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoft_labels \u001b[39m=\u001b[39m f_data_to_soft_labels(\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=412'>413</a>\u001b[0m     batch[\u001b[39m\"\u001b[39;49m\u001b[39mtokenized_first_sentence\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=413'>414</a>\u001b[0m     batch[\u001b[39m\"\u001b[39;49m\u001b[39mtokenized_next_50_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=414'>415</a>\u001b[0m     num_predictions_max\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=415'>416</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=416'>417</a>\u001b[0m \u001b[39m# check that the mask makes sense!\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=417'>418</a>\u001b[0m mask \u001b[39m=\u001b[39m (batch[\u001b[39m\"\u001b[39m\u001b[39mtokenized_next_50_tokens\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m50256\u001b[39m)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;32m/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb Cell 34\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Extract the predictions corresponding to B\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(lengths_A)):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     hold_output[i, :lengths_B[i], :] \u001b[39m=\u001b[39m predictions[i, lengths_A[i]:lengths_A[i] \u001b[39m+\u001b[39;49m lengths_B[i], :]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sidbaskaran/Desktop/research/share/hypernetworks/editing_wikipedia_sentences.ipynb#X45sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mreturn\u001b[39;00m hold_output\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m50257\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/interp/lib/python3.10/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
=======
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200:   0%|          | 1/30864 [06:24<3298:03:38, 384.70s/batch]"
>>>>>>> 74203ae363958b1962be144bcd83036c8e28ea2a
     ]
    }
   ],
   "source": [
    "#current problem: 1728/ 30864 \n",
    "hypernetwork.train(\n",
    "    train_loader=data_loader,\n",
    "    test_loader=test_data_loader,\n",
    "    stop_editing_index=stop_editing_index,\n",
    "    epochs=200,\n",
    "    lam = 0,#20000\n",
    "    lam_testing_penalty = 0,\n",
    "    KL_divergence_loss=True,\n",
    "    f_data_to_soft_labels = f_data_to_soft_labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now look at some outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
=======
   "source": [
    "#hypernetwork = torch.load(\"checkpoint_epoch_0_batch_8811.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m     temp \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mhypernetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minspect_batch_prediction_ouptuts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 232\u001b[0m, in \u001b[0;36mEditorHypernetwork.inspect_batch_prediction_ouptuts\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meditor_inputs,\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     output_editor_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    230\u001b[0m )\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m#compute most likely tokens from the logits\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m predicted_ids \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39margmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpredictions\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m#convert the token ids to strings\u001b[39;00m\n\u001b[1;32m    234\u001b[0m predicted_strings \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(pred) \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predicted_ids]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
>>>>>>> 74203ae363958b1962be144bcd83036c8e28ea2a
   "source": [
    "for batch in data_loader:\n",
    "    temp = batch\n",
    "    break\n",
    "hypernetwork.inspect_batch_prediction_ouptuts(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in data_loader:\n",
    "    result = hypernetwork.forward(\n",
    "        editor_input_ids=batch[\"tokenized_first_sentence\"],\n",
    "        target_input_ids=batch[\"tokenized_next_50_tokens\"],\n",
    "        stop_editing_index=stop_editing_index,\n",
    "        output_target_hidden_states=True,\n",
    "        output_edited_hidden_states=True,\n",
    "        output_edit_vectors=True,\n",
    "        output_editor_attention=True,\n",
    "    )\n",
    "    temp=batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m predicted_strings\u001b[38;5;241m=\u001b[39m\u001b[43mhypernetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minspect_batch_prediction_ouptuts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 232\u001b[0m, in \u001b[0;36mEditorHypernetwork.inspect_batch_prediction_ouptuts\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meditor_inputs,\n\u001b[1;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    229\u001b[0m     output_editor_attention\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    230\u001b[0m )\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m#compute most likely tokens from the logits\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m predicted_ids \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39margmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpredictions\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m    233\u001b[0m \u001b[38;5;66;03m#convert the token ids to strings\u001b[39;00m\n\u001b[1;32m    234\u001b[0m predicted_strings \u001b[38;5;241m=\u001b[39m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(pred) \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predicted_ids]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
     ]
    }
   ],
   "source": [
    "predicted_strings=hypernetwork.inspect_batch_prediction_ouptuts(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46172, 21071, 3754, 6711, 357, 66, 13]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = 0\n",
    "for i in df[\"tokenized_first_sentence\"]:\n",
    "    temp2 = i\n",
    "    y = y + 1\n",
    "    if(y ==2000):\n",
    "        break\n",
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypernetwork.evaluate_KL_test_loss_nogradient(data_loader, f_data_to_soft_labels, stop_editing_index=stop_editing_index)\n",
    "# #hypernetwork.inspect_batch_prediction_ouptuts(temp)\n",
    "result= hypernetwork.forward(\n",
    "    editor_input_ids=temp[\"tokenized_first_sentence\"],\n",
    "    target_input_ids=temp[\"tokenized_next_50_tokens\"],\n",
    "    stop_editing_index=stop_editing_index,\n",
    "    output_target_hidden_states=True,\n",
    "    output_edited_hidden_states=True,\n",
    "    output_edit_vectors=True,\n",
    "    output_editor_attention=True,\n",
    ")\n",
    "#compute most likely tokens from the logits\n",
    "predicted_ids = [torch.argmax(pred, dim=-1) for pred in result[\"logits\"]]\n",
    "#convert the token ids to strings\n",
    "predicted_strings = [tokenizer.decode(pred) for pred in predicted_ids]\n",
    "\n",
    "#Compute the most likely tokens from running gpt2 on the target_input_ids\n",
    "gpt2_result = model(temp[\"tokenized_next_50_tokens\"].cuda())\n",
    "gpt2_predicted_ids = [torch.argmax(pred, dim=-1) for pred in gpt2_result[\"logits\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenized_first_sentence': tensor([[  464,  6387,   829, 24523,  3834, 21426,   318,   257, 24337,  1627,\n",
       "           6898,   416,  9429,    55, 15198,   287,   262,   471,    13,    50,\n",
       "             13, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464,  3517,  7963, 10749,   318,   257,  5701,  1097, 11717,  2168,\n",
       "           1912, 20736,   287,   262,  1578,  7526,    13, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [ 1925, 31131,  8466, 25655, 36749,  7484,   318,   257,  4693,   286,\n",
       "          40941,  4077,  4580,   282,  4908,   287,   262,  7458,   609,  4685,\n",
       "          16982,  8326,    13, 50256, 50256, 50256],\n",
       "         [ 5653, 14620,    11, 11419,  2714,  1877,    12,  6477,  5581, 16625,\n",
       "            290,  5103, 13892,   287,  1811,   471,    13,    50,    13, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [43339, 12398, 38174,   357,  6286,  3945,  2242,    11,  9656,     8,\n",
       "            318,   257,  1966,  4708,  5398,  4346, 18961,    13, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [ 5990, 12894,   261,   393,  2448, 12894,    78,   357,  2704,    13,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [   39,  1211,  1525, 12950,  1322,   318,   257,  3517,  2746, 45247,\n",
       "           9138,  1664,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [   45,   533, 24631,  1869, 14403,  7499,   357,  1129,  3365,    30,\n",
       "             12,  1983,  3389,  3717,     8,   373,   257, 15310,  2040,    68,\n",
       "          44185,    13, 50256, 50256, 50256, 50256],\n",
       "         [  464, 23948,   286, 12457,  4951,   544,   318,   257, 10530,  3194,\n",
       "            416, 18623, 39043,   290,  5780,  4243,  4223,   380,    13, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [10082,  2364,  4364,  1982, 15739, 27428,   357,  6286,   352,  3269,\n",
       "          20033,     8,   318,   257,  1751,   338, 37986,   290, 21810,    13,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464, 12863,    14,  4521,   376,  1797, 38809, 15903,   278,  2159,\n",
       "           5454,   373,   262,   767,   400,  2159,  5454,  1622,   287, 19984,\n",
       "          14284,    13, 50256, 50256, 50256, 50256],\n",
       "         [ 9861,  1734,    64,   299,   425,    64,   318,   257,  4693,   286,\n",
       "          45489,   287,   262,  1641, 17419,   321,  1525,    66, 31718,    13,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464, 11937,   286,  3942, 16712,   357,    37,  3539,     8,   318,\n",
       "            281, 18091,  2831,  1767,   287,  3794,    13, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [   54,   648,   402,  3702,   648,   357,    26,  1737,  2579,    11,\n",
       "          41435,  1849,  1906,  3426,   838,    11,  7795,     8,   373,   257,\n",
       "           3999,  4523, 33013,    13, 50256, 50256],\n",
       "         [10445, 31047,   357, 36234,    11,  2258, 39812,     8,   318,   281,\n",
       "            555,  1939, 40132,  2055,   287, 43178,   292,  3418,    11,  3442,\n",
       "             13, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464,  5618,  6188, 28403,  8997, 45065, 12713,    12,    18,    11,\n",
       "            357, 11674,  3705,   350, 10526, 12713,    12,    18,   828,   318,\n",
       "            257,   471,    13,    50,    13, 50256],\n",
       "         [ 3791,   367,   622,    64,  1134,  3832,   318,   257,  7404,   287,\n",
       "            262,   609, 28474,  1872,  4783,   286, 29975,   273,   321,    11,\n",
       "           3794,    13, 50256, 50256, 50256, 50256],\n",
       "         [39989, 22864,   357,  6286,  3035,  2608,    11, 28017,     8,   318,\n",
       "            257, 29570,   287, 20383,   313,  1435,   290,   262, 21546, 14733,\n",
       "           3356,    13, 50256, 50256, 50256, 50256],\n",
       "         [32476,   370,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464,  2681,   400,  3195,    88,  2949,   626,   292, 15434,   318,\n",
       "            281,  8581,   286,  2041, 13304,   284,   262,  1266,   286, 19533,\n",
       "           1515,   292,   290,  3195,  2523,    13],\n",
       "         [   56,   897, 33110,   357,  6286,  2808,  1737,  7358,     8,   318,\n",
       "            257,  5398,  1067,   624,  2357,    13, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464,  3834, 32679,  9870,   290, 40947,  5018, 11819,   318,   257,\n",
       "           3331,  2615,   379, 48777,   311,    13, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [32434, 15796, 12181,   357,    66,    13, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [   47,   292,   270, 21067,   837,   635,  1900,   355,   837,   318,\n",
       "            257, 12175,  9526, 21388, 11210,   286, 22721,    13, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [ 2202,  2608,  1737,  3050,    11,   379,  1551,  3598,   661,   547,\n",
       "           2923,   287,   257,  5194, 11975,   287,   520,   615,  1773,   349,\n",
       "             11,  3284,    13, 50256, 50256, 50256],\n",
       "         [43568,   367,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464,  2671,  6005, 40204,   318,   257,  2099,   286, 40204,  6317,\n",
       "             13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [35649,   265,   623,  3247,  8130,  5665,   318,   257,  4783,   287,\n",
       "            262, 42682,   313,   430,    12,    44,   648, 16522, 17718,   286,\n",
       "          46694,    13, 50256, 50256, 50256, 50256],\n",
       "         [   45,  1077,  4244,   318,   262,  3139,   290,   749, 44042,  1748,\n",
       "            286,   262,   471,    13,    50,    13, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [   51,  2049, 33487,   318,   281,   555,  1939, 40132,  2055,   287,\n",
       "          29539,  3418,    11,   287,   262,   471,    13,    50,    13, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [ 5841,   395,    78, 20000,  5535,   357,    44, 34382,     8,   318,\n",
       "            257,  1171,  2055,  4152,   287, 42810,    78,    11,  3442,    13,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256],\n",
       "         [  464, 29290, 12758,   797, 11218,  1400,    13, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
       "          50256, 50256, 50256, 50256, 50256, 50256]], device='cuda:0',\n",
       "        dtype=torch.int32),\n",
       " 'tokenized_next_50_tokens': tensor([[ 9012,   286,  9266,  ..., 50256, 50256, 50256],\n",
       "         [  464,  2168,   373,  ...,   350,   557, 15516],\n",
       "         [ 1026,   468,   257,  ..., 50256, 50256, 50256],\n",
       "         ...,\n",
       "         [ 5219,   286, 11867,  ..., 50256, 50256, 50256],\n",
       "         [ 1026,   318,   636,  ..., 50256, 50256, 50256],\n",
       "         [   16,    11,   393,  ...,  1660,  3608,   415]], device='cuda:0',\n",
       "        dtype=torch.int32)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preceding string:  ['The', ' Boy', 'les', ' Terminal', ' Sub', 'division', ' is', ' a', ' railroad', ' line', ' owned', ' by', ' CS', 'X', ' Transportation', ' in', ' the', ' U', '.', 'S', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', ' the', ' the', ' Alabama', '\\n', ' state', ' from', ' from', ' the', ' Castle', ',', ',', ',', ' to', ',', ',', ',', ' Alabama', ' Alabama', ' of', ' of', ' of', '.', ',', ',', ',', ' to', ' to', ' to', ' to', ' to', ' Castle', ' Castle', ' Castle', ' Castle', ' the', ' the', ' the', ' the', ' the', ' the', ' to', ' to', ' to', ' to', ' to']\n",
      "GPT2 argmax:  ['.', ' the', ',', '\\n', ' state', ' is', ' from', ' the', ' York', ' to', ' Ala', ',', ' to', ' the', ',', ' Alabama', ',', ' and', ' about', ' total', ' of', ' $', ' miles', '5', ' miles', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['State', ' of', ' Alabama', '.', ' The', ' line', ' runs', ' from', ' New', ' Castle', ',', ' Alabama', ',', ' to', ' Hoover', ',', ' Alabama', ',', ' for', ' a', ' total', ' of', ' 17', '.', '3', ' miles', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' British', ' GT', ' Championship', ' is', ' a', ' sports', ' car', ' racing', ' series', ' based', ' predominantly', ' in', ' the', ' United', ' Kingdom', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' of', ' is', ' created', ' created', ' by', ' the', ' company', ' Racing', ' Racing', ',', ',', ',', ' and', ' for', ' for', ' for', ' for', ' for', ' the', ' the', ' the', ' the', ' the', ',', ' the', ',', ',', ',', ',', ' the', ' the', ' the', ' the', ' by', ' the', ' by', ' by', ',', '.', ',', ',']\n",
      "GPT2 argmax:  ['\\n', ' is', ' originally', ' scheduled', ' by', ' the', ' creators', ' television', ' League', \"'\", ' Association', ' and', ' the', '.', ' has', ' in', ' the', ' first', ' season', ' seasons', ',', ' was', ' produced', ' as', ' the', ' \"', ' Racing', ' Car', ' Series', '.', '\\n', ' series', ' was', ' now', ' in', ' by', ' the', ' British', 'irling', 'ph', 'ane', ' D', 'l', 'le', ',', ' which', ' the', 'ire', 'lli', ' is']\n",
      "Actual string:  ['The', ' series', ' was', ' originally', ' created', ' by', ' the', ' British', ' Racing', ' Drivers', \"'\", ' Club', ' in', ' 1993', ' and', ',', ' for', ' its', ' first', ' two', ' seasons', ',', ' was', ' known', ' as', ' the', ' National', ' Sports', ' GT', ' Challenge', '.', ' The', ' series', ' is', ' currently', ' run', ' by', ' the', ' St', '', 'ph', 'ane', ' Rate', 'l', ' Organisation', ',', ' while', ' P', 'ire', 'lli']\n",
      "\n",
      "\n",
      "Preceding string:  ['Ch', 'lore', 'lla', ' sor', 'okin', 'iana', ' is', ' a', ' species', ' of', ' freshwater', ' green', ' micro', 'al', 'ga', ' in', ' the', ' Division', ' Ch', 'lor', 'ophy', 'ta', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', \"'s\", ' been', ' very', ' of', 'ald', '-', 'green', ' green', ' and', ' color', '.', '.', '.', '.', ' cells', ' to', ' to', ' to', ' to', ' to', ' cells', ' cells', ' cells', ' to', ' to', ' hours', ' to', '\\n', '\\n', ' color', '\\n', 'color', 'color', 'color', 'color', 'color', 'color', 'Color', 'Color', 'Color', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "GPT2 argmax:  [' is', ' been', ' lot', ' of', 'gent', ' green', 'green', ' hue', ',', ' is', ' to', 'y', '.', ' It', ' leaves', ' are', ' into', ' and', ' form', ' a', ' to', ' cells', '.', ' day', ' days', ' 24', ' hours', '.', ' The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' has', ' a', ' characteristic', ' emer', 'ald', '-', 'green', ' color', ' and', ' pleasant', ' grass', ' odor', '.', ' Its', ' cells', ' divide', ' rapidly', ' to', ' produce', ' four', ' new', ' cells', ' every', ' 17', ' to', ' 24', ' hours', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['MS', ' Communications', ',', ' LLC', ' held', ' low', '-', 'power', ' television', ' licenses', ' and', ' construction', ' permits', ' in', ' several', ' U', '.', 'S', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', ' the', ' the', ' year', 's', '\\n', '\\n', ' the', ' the', ' the', ' the', ' of', ' the', ' the', ' the', '-', ' highest', ' number', ' of', ' of', ' of', ',', ',', ',', ' States', ' States', ',', ' the', ' of', ' of', ' of', ',', ',', ',', ' the', ' the', ' the', ' the', ' the', ' the', ' the', ' point', ' point', ' point']\n",
      "GPT2 argmax:  ['.', ' the', ' last', 's', '.', '\\n', '\\xa0', ' the', ' point', ',', 'U', ' was', ' a', ' first', ' largest', 'largest', ' number', ' of', ' subscribers', 'N', 's', ' in', ' the', ' country', ' States', '.', ' with', ' it', ' half', ' of', ' the', ' other', ' were', ' in', 'sold', '.', ' under', '-', ' ', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['states', ' during', ' the', ' 2000', 's', '.', ' ', ' At', ' one', ' point', ' MS', ' Communications', ' held', ' the', ' second', '-', 'highest', ' number', ' of', ' LP', 'TV', ' allocations', ' in', ' the', ' United', ' States', ',', ' but', ' nearly', ' all', ' of', ' the', ' stations', ' remained', ' un', 'built', ' or', ' dark', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Mit', 'chell', ' Barnett', ' (', 'born', ' February', ' 23', ',', ' 1993', ')', ' is', ' a', ' former', ' professional', ' Canadian', ' football', ' linebacker', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' was', ' born', ' by', 'th', ' overall', ' in', ' the', ' 2016', ' Draft', ' Draft', ' Draft', ' the', ',', '-', '-', ' and', ' the', ' the', ' with', ' with', ' the', ' the', ' the', '.', ',', '.', '.', ' seasons', ' seasons', ' seasons', ' and', ' and', ' the', ' and', '-', ' the', ' the', ' and', ' the', ' the', ' the', ' the']\n",
      "GPT2 argmax:  [' is', ' a', ' by', 'th', ' overall', ' by', ' the', ' 2012', ' NFL', ' Draft', ' by', ' the', ' Calgary', ' Tiger', '-', 'C', 'ats', '.', ' was', ' with', ' the', ' Hamilton', ' in', ' July', ' 1', ',', ' 2016', '.', ' He', ' a', ' seasons', ' with', ' two', ' starts', ' with', ' the', ' Bom', '-', 'C', 'ats', ',', ' he', ' was', ' with', ' a', ' free', ' agent', ' with', ' the', ' Toronto']\n",
      "Actual string:  ['He', ' was', ' drafted', ' 59', 'th', ' overall', ' in', ' the', ' 2016', ' CFL', ' Draft', ' by', ' the', ' Hamilton', ' Tiger', '-', 'C', 'ats', ' and', ' signed', ' with', ' the', ' team', ' on', ' May', ' 27', ',', ' 2016', '.', ' After', ' two', ' seasons', ' and', ' 29', ' games', ' with', ' the', ' Tiger', '-', 'C', 'ats', ',', ' he', ' signed', ' as', ' a', ' free', ' agent', ' with', ' the']\n",
      "\n",
      "\n",
      "Preceding string:  ['Per', 'dig', 'on', ' or', ' Per', 'dig', 'o', ' (', 'fl', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' SHARES', ' SHARES', ' SHARES', '\\n', '\\n', '\\n', ' the', ' a', ' the', 'our', 'our', 'our', 'our', 'our', 'ron', 'ron', 'ron', ',', 'an', ',', ',', 'teen', 'an', 'an', 'an', 'on', 'on', 'on', 'on', 'on', 'on', 'on', 'ron', 'ron']\n",
      "GPT2 argmax:  ['.', '.', '10', '09', '.', '\\n', ' found', ' major', 'bad', 'our', ' who', ' the', 'ille', '', 'd', ',', ' the', ' late', 'are', 'ral', '', 'an', ' region', ' He', ' years', ' years', ' the', ' companions', ' were', '.', ' including', ' the', ' in', ' of', ',', ' a', ' and', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['119', '0', '', '12', '20', ')', ' was', ' a', ' trou', 'bad', 'our', ' from', ' L', 'esp', '', 'ron', ' in', ' the', ' G', '', 'v', 'aud', 'an', '.', ' Four', 'teen', ' of', ' his', ' works', ' survive', ',', ' including', ' three', ' cans', 'os', ' with', ' melodies', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['H', 'orn', 'by', ' Rail', 'ways', ' is', ' a', ' British', ' model', ' railways', ' manufacturing', ' company', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' are', ' are', ' back', ' to', ' to', ',', ' the', ',', ',', ',', ',', ',', ' the', ',', ' the', ' for', ' the', ' the', 'ano', '.', '.', '.', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for', ' for']\n",
      "GPT2 argmax:  [',', ' in', ' back', ' to', ' the', ',', ' the', ',', ' where', ' a', ' of', ' Lloyd', ' was', ' was', ' a', ' letter', ' for', ' a', ' invention', 'yers', 'an', '-', '.', '.', ' The', ' company', ' Me', ' was', ' clock', ' was', ' built', ' in', ' 18', ',', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['Its', ' roots', ' date', ' back', ' to', ' 1901', ' in', ' Liverpool', ',', ' when', ' founder', ' Frank', ' Horn', 'by', ' received', ' a', ' patent', ' for', ' his', ' Me', 'cc', 'ano', ' construction', ' toy', '.', ' The', ' first', ' clock', 'work', ' train', ' was', ' produced', ' in', ' 1920', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['N', 'are', 'ndra', ' Man', ' Singh', ' ()', ' (', '19', '58', '?', '-', '27', ' November', ' 2009', ')', ' was', ' a', ' Nep', 'ales', 'e', ' footballer', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', \"'s\", ' was', ' first', ' man', 'ali', ' to', ' to', ' win', ' the', ' football', ' in', ',', ',', ',', ' for', ' for', ' for', ' the', ' team', ' team', ' team', ' the', ' Games', ' Games', '.', '.', ' played', '.', ' years', ' years', ' years', ' in', ' for', ' the', ' the', ' the', ',', 'al', ',', 'al', ' and', ' and', ' and', ' and', '.', ' the']\n",
      "GPT2 argmax:  [' is', ' a', ' first', ' person', 'ales', ' to', ' to', ' play', ' for', ' football', ' in', ' the', '.', ' and', ' he', ' for', ' the', ' Indian', ' national', ' team', '.', ' the', ' 2010', ' World', ' Cup', '.', ' He', ' was', ' for', ' every', ' years', ' for', ' the', ' national', ' teams', ' teams', ',', 'ys', 'ra', ' and', ' and', ' and', ' and', ' the', 'an', 'ra', ' Group', ' SC', '\\n', 'The']\n",
      "Actual string:  ['He', ' was', ' the', ' first', ' Nep', 'ali', ' player', ' to', ' play', ' professional', ' football', ' in', ' India', ',', ' and', ' played', ' for', ' the', ' Nepal', ' national', ' team', ' in', ' the', ' 1982', ' Asian', ' Games', '.', ' He', ' played', ' nearly', ' eight', ' years', ' for', ' the', ' two', ' Indian', ' sides', ' M', 'af', 'atl', 'al', ' Group', ' SC', ' and', ' Mah', 'ind', 'ra', ' United', '..', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' Fields', ' of', ' Amb', 'ros', 'ia', ' is', ' a', ' musical', ' written', ' by', ' Joel', ' Higgins', ' and', ' Martin', ' Sil', 'vest', 'ri', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', \"'s\", ' a', ' by', ' the', ' UK', ' Park', ' Park', ',', ',', ',', ',', ',', ',', ' in', ' in', ' in', ' was', ' was', ' was', ' by', ' by', ' by', ' by', ' by', ' by', ' by', ' by', ' by', ' by', ' by', ',', '-', ' by', ' by', ' by', ' by', ' by', ',', ',', ' by', ',', ' by', ' by']\n",
      "GPT2 argmax:  [' is', ' a', ' by', ' the', ' same', ' Washington', ' area', 'house', ',', ' the', ' York', ',', ' Canada', ' Brunswick', '.', ' the', '.', ' was', ' was', ' performed', ' by', ' John', ' Peck', 'ley', '.', ' who', 'ographed', ' by', ' John', 'ne', ' St', '.', 'Johnson', 'bett', '.', ' and', ' by', ' the', 'st', ' and', ' and', ' performed', ' in', ' by', ' John', ' L', 'inski', '.', '\\n', ' play']\n",
      "Actual string:  ['It', ' was', ' performed', ' in', ' the', ' George', ' Street', ' Play', 'house', ' in', ' New', ' Brunswick', ',', ' New', ' Jersey', ' in', ' 1993', ' and', ' it', ' was', ' directed', ' by', ' Gregory', ' Hur', 'st', ',', ' chore', 'ographed', ' by', ' Lyn', 'ne', ' Taylor', '-', 'Cor', 'bett', ',', ' staged', ' by', ' Hur', 'st', ',', ' and', ' set', ' design', ' by', ' Deborah', ' Jas', 'ien', '.', ' The']\n",
      "\n",
      "\n",
      "Preceding string:  ['Ge', 'off', 'rey', ' Mc', 'Sk', 'imming', ' (', 'born', ' 1', ' January', ' 1962', ')', ' is', ' a', ' children', \"'s\", ' novelist', ' and', ' poet', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', \"'s\", ' a', ' first', ' of', ' the', ' \"', 'th', ' book', ',', ' and', 'icles', ' and', ' and', ' and', ',', ',', 'aunts', ' and', ' the', ' the', ' the', ' the', ' and', ' of', ' of', ' of', '.', '.', '.', '.', '.', ' is', ' is', ' the', ',', ',', 's', '\\n', '\\n', ' \"', ' \"', ' book', ' book', ' book']\n",
      "GPT2 argmax:  [' is', ' a', ' only', ' of', ' the', ' book', 'th', ' series', ':', 'bo', 'icle', ' the', ' the', 'ining', 'lyn', \"'s\", 'born', \"'s\", 'im', ' through', ' travels', ' author', '.', 'is', ' Schl', ' of', '.', ' books', ' novels', '.', '\\n', ' is', ' written', ' written', ' the', ' books', ' of', ' the', ',', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['He', ' is', ' the', ' author', ' of', ' the', ' 20', ' volume', ' Cairo', ' Jim', ' chron', 'icles', ' and', ' Jo', 'ce', 'lyn', ' Os', 'good', ' j', 'aunts', ' and', ' the', ' Ph', 'yll', 'is', ' Wong', ' series', ' of', ' mystery', ' novels', '.', ' He', ' has', ' also', ' published', ' three', ' volumes', ' of', ' poetry', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' 1985', '/', '86', ' F', 'IS', ' Ski', ' Jump', 'ing', ' World', ' Cup', ' was', ' the', ' 7', 'th', ' World', ' Cup', ' season', ' in', ' ski', ' jumping', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', \"'s\", \"'s\", ' the', ',', ',', ',', ' on', ' the', ' March', ' and', ' in', ' in', ' in', ',', 'ica', ',', ',', ',', '.', '.', '.', '.', '.', ' World', ' World', ' World', ' by', ' by', ' by', ' by', '', '', '', '', '', '', '.', ' by', ',', ',', '\\n', ' Wednesday', ' Wednesday', ' Wednesday', ' Wednesday', ' Wednesday']\n",
      "GPT2 argmax:  [' is', ' with', ' the', ' Bay', ',', ' Ontario', ',', ' July', ' July', ',', '.', ' continued', ' in', ' the', 'o', ',', ' New', ' on', ' 8', ' December', ' 1986', '.', '\\n', ' first', ' was', ' Cup', ' winners', ' held', ' by', ' the', 'ias', ',', 'gaard', '', 'nen', ',', ' the', ' Cup', ' by', ' J', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' began', ' in', ' Thunder', ' Bay', ',', ' Canada', ' on', ' 7', ' December', ' 1985', ' and', ' finished', ' in', ' Plan', 'ica', ',', ' Yugoslavia', ' on', ' 23', ' March', ' 1986', '.', ' The', ' individual', ' World', ' Cup', ' was', ' won', ' by', ' Matt', 'i', ' Ny', 'k', '', 'nen', ' and', ' Nations', ' Cup', ' by', ' Austria', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Gl', 'ene', 'a', ' n', 'ive', 'a', ' is', ' a', ' species', ' of', ' beetle', ' in', ' the', ' family', ' Cer', 'am', 'by', 'c', 'idae', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', \"'s\", ' a', ' as', ' the', '.', 'se', 'ma', ' as', ' as', ':', ',', ' It', ':', ':', ':', ' The', ' The', ' The', ' The', ' The', ' The', ' The', ' The', ' The', ' The', '\\n', '\\n', '\\n', '\\n', '\\n', ' was', ' a', ' in', \"'s\", ' of', ' of', ' of', ' of', ' of', ' of', ' of', ' of', ' of', ' of']\n",
      "GPT2 argmax:  [' is', ' a', ' as', ' the', '.', 'ika', 'v', ' as', ' a', '48', ' as', '\\n', ' was', ' said', ' as', ' the', ' as', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' was', ' described', ' by', ' R', 'it', 'se', 'ma', ' in', ' 18', '92', '.', ' It', ' is', ' known', ' from', ' Indonesia', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' Federation', ' of', ' Indian', ' Airlines', ' (', 'F', 'IA', ')', ' is', ' an', ' airline', ' industry', ' body', ' in', ' India', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' are', ' are', ' a', 'i', 'i', ',', ',', ' and', ' and', ' and', ' and', '\\n', '\\n', '\\n', '\\xa0', ' of', ' of', ' by', ' are', ' by', ' by', ' by', ' by', ' by', ' of', ' of', ' of', ' of', ' by', ' are', ' are', ' are', ' the', '\\n', '\\n', '\\xa0', '\\xa0', '\\n', '\\n', '\\n', '\\n']\n",
      "GPT2 argmax:  [',', ' are', ' also', 'epend', 'Go', ',', ' the', ',', ',', ' Jet', 'Air', '.', '\\n', '\\xa0', ' company', ' of', ' the', ' companies', ' are', ' to', ' out', ' by', ' the', ' FIA', ' Board', ' of', ' of', ' the', ' FIA', ' of', ' the', ' of', ' the', ' three', ' associations', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['Its', ' members', ' are', ' Ind', 'i', 'Go', ',', ' Spice', 'Jet', ' and', ' Go', 'Air', '.', ' ', ' The', ' functions', ' of', ' the', ' FIA', ' are', ' carried', ' out', ' by', ' an', ' Executive', ' Council', ' composed', ' of', ' the', ' heads', ' of', ' each', ' of', ' the', ' member', ' airlines', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['W', 'ang', ' G', 'anch', 'ang', ' (', ';', ' May', ' 28', ',', ' 1907', '\\xa0', '', ' December', ' 10', ',', ' 1998', ')', ' was', ' a', ' Chinese', ' nuclear', ' physicist', '.', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', \"'s\", ' a', ' of', ' the', ' most', ' fathers', ' of', ' the', ' the', ' physics', ',', ',', ' rays', ' and', ' particles', ' particle', '.', '.', ' was', ' was', ' a', ' the', ' the', ' the', ' the', ' of', ' of', ' of', ' of', ' experiments', ' experiments', ' experiments', '-', '-', '-', '-', '-', ' electromagnetic', ',', ',', ' nuclear', ' nuclear', ',', ' nuclear', '-', '-', ' nuclear', ' nuclear']\n",
      "GPT2 argmax:  [' is', ' a', ' of', ' the', ' first', ' members', ' of', ' the', ' culture', ' power', ',', ' and', ' rays', ',', ' the', ' physics', '.', ' He', ' was', ' also', ' a', ' member', ' in', ' the', ' development', ' of', ' physics', 'ation', ' and', ' and', ' and', ' and', '-', 'gravity', 'rom', 'agnetic', ' and', ' experiments', ' and', ' and', ' physics', ' physics', ' and', ' and', '-', 'nuclear', ' weapons', ',', ',', ' and']\n",
      "Actual string:  ['He', ' was', ' one', ' of', ' the', ' founding', ' fathers', ' of', ' Chinese', ' nuclear', ' physics', ',', ' cosmic', ' rays', ' and', ' particle', ' physics', '.', ' Wang', ' was', ' also', ' a', ' leader', ' in', ' the', ' fields', ' of', ' deton', 'ation', ' physics', ' experiments', ',', ' anti', '-', 'elect', 'rom', 'agnetic', ' pulse', ' technology', ',', ' nuclear', ' explosion', ' detection', ',', ' anti', '-', 'nuclear', ' radiation', ' technology', ',']\n",
      "\n",
      "\n",
      "Preceding string:  ['Sen', 'eca', ' (', 'formerly', ',', ' North', ' Fork', ')', ' is', ' an', ' un', 'inc', 'orporated', ' community', ' in', ' Plum', 'as', ' County', ',', ' California', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', \"'s\", ' in', ' the', ' intersection', ' of', ',', ' feet', ' feet', ' feet', '.', '.', '.', '.', '.', ' is', ' is', ' the', ' the', ' the', ' the', ' the', ' River', ' River', ' of', ' of', ' on', ',', '\\n', '\\n', '\\n', '1', '1', 'feet', 'feet', ' feet', ' feet', ' feet', ' feet', ' feet', ' feet', ' feet', ' feet', ' feet', ' feet', ' feet']\n",
      "GPT2 argmax:  [' is', ' in', ' the', ' intersection', ' of', ' about', ',', ' feet', '.', '1', '4', ' meters', ')', ' The', 'eca', \"'s\", ' the', ' at', ' the', ' south', ' side', ' of', ' River', ',', ' about', '\\xa0', ' of', ' the', ' Creek', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' lies', ' at', ' an', ' elevation', ' of', ' 36', '25', ' feet', ' (', '110', '5', ' m', ').', ' Sen', 'eca', ' is', ' located', ' on', ' the', ' North', ' Fork', ' Feather', ' River', ',', ' ', ' north', ' of', ' Twain', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' Federal', ' Information', ' Processing', ' Standard', ' Publication', ' 140', '-', '3', ',', ' (', 'FI', 'PS', ' P', 'UB', ' 140', '-', '3', '),', ' is', ' a', ' U', '.', 'S', '.', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', '\\n', ' security', ' security', ' is', ' to', ' use', ' security', ' and', '.', '\\n', ' the', ' is', ' Security', ' Requirements', ' for', ' Security', ' Crypt', ' Security', 'ules', '.', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n']\n",
      "GPT2 argmax:  [',', ' system', ' system', '.', ' by', ' protect', ' the', ' keys', ' for', '\\n', ' standard', ' of', ' a', ' and', ' for', ' the', 'ographic', ' Module', 'ules', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['government', ' computer', ' security', ' standard', ' used', ' to', ' approve', ' cryptographic', ' modules', '.', ' The', ' title', ' is', ' Security', ' Requirements', ' for', ' Crypt', 'ographic', ' Mod', 'ules', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['New', ' H', 'ru', 'a', 'ik', 'awn', ' is', ' a', ' village', ' in', ' the', ' Ch', 'amph', 'ai', ' district', ' of', ' Miz', 'or', 'am', ',', ' India', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', \"'s\", ' a', ' in', ' the', ' heart', 'aw', 'b', 'ung', '.', '.', '.', '.', '.', '.', '.', '.', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', 'A', 'A', 'The', 'The', 'The', 'The', 'A', 'A', 'A', 'A', 'A']\n",
      "GPT2 argmax:  [' is', ' not', ' in', ' the', ' heart', 'ar', 'aja', ' area', 'a', 'atch', 'A', '.', ' area', ',', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' is', ' located', ' in', ' the', ' Kh', 'aw', 'b', 'ung', ' R', '.', 'D', '.', ' Block', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Allen', ' Klein', ' (', 'born', ' April', ' 26', ',', ' 1938', ')', ' is', ' a', ' pioneer', ' in', ' gel', 'ot', 'ology', ' and', ' the', ' therapeutic', ' humor', ' movement', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', ',', ',', ',', ',', \"'s\", ' married', ' married', ' a', ' years', ' when', ' when', ' she', ' of', ' of', ' of', ' of', ',', ',', ' and', ' the', ' her', ' had', ' had', ' had', ' of', ' of', ' of', ' of', ' the', ' to', ' to', ' to', ' the', ' the', ' to', ' to', ' to', ' to', ' to', ' career', ' career', ' career', ' career', ' career', ' the', ' and']\n",
      "GPT2 argmax:  [' the', ',', ' the', ' was', ' book', ',', ' diagnosed', ' a', ' years', ' old', '.', ' she', ' died', '.', ' cancer', ' cancer', '.', ' and', ' she', ' couple', ' of', ' she', ' was', ' been', ' her', ' liver', ' of', ' humor', ' and', ' her', ' way', ' through', ' the', ' end', ' of', ' her', ' to', ' write', ' up', ' her', ' job', ' job', ' as', ' a', ' writer', ' director', ' film', ' director', ' writer', '.']\n",
      "Actual string:  ['In', ' 1974', ',', ' Klein', \"'s\", ' wife', ' was', ' only', ' 34', ' years', ' old', ' when', ' she', ' died', ' of', ' liver', ' disease', ',', ' and', ' the', ' aspect', ' where', ' she', ' had', ' kept', ' her', ' sense', ' of', ' humor', ' all', ' the', ' way', ' to', ' the', ' end', ' inspired', ' Klein', ' to', ' give', ' up', ' his', ' previous', ' career', ' as', ' a', ' theater', ' and', ' television', ' scene', ' designer']\n",
      "\n",
      "\n",
      "Preceding string:  ['Henry', ' W', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', '\\n', '\\xa0', '\\xa0', ',', ',', ')', ' ', ' ', ')', ',', ',', ')', ' a', ',', ',', ' was', ' was', ' was', ' was', ' was', ' was', ' was', ' was', ' was', ' was', ' was']\n",
      "GPT2 argmax:  [',', ',', '\\xa0', '1', ' 2010', ',', ' 2012', ')', ' December', ' 8', ',', ' 18', ')', ' a', ' a', ' member', ' wrestler', ' player', ' who', ' played', ' for', ' base', ' for', ' the', ' New', ' League', 'agues', '.', ' the', ' New', '76', ' and', '.', ' Louis', ' Cardinals', '.', ' He', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['Full', 'er', ' ', ' (', 'December', ' 5', ',', ' 1862', ' ', ' December', ' 12', ',', ' 1895', '),', ' was', ' a', ' professional', ' baseball', ' player', ' who', ' played', ' third', ' base', ' in', ' the', ' Major', ' Le', 'agues', ' for', ' the', ' 18', '91', ' St', '.', ' Louis', ' Browns', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' 27', 'th', ' TV', 'y', 'No', 'vel', 'as', ' Awards', ' is', ' an', ' Academy', ' of', ' special', ' awards', ' to', ' the', ' best', ' of', ' soap', ' oper', 'as', ' and', ' TV', ' shows', '.']\n",
      "Predicted argmax:  [' first', ' show', ' was', ' place', ' on', ' the', ' 11', ',', ' 2017', ' at', ' the', ' city', ' of', 'o', ' in', ' Palace', ' the', 'ap', 'ul', 'co', ',', ' Mexico', ',', ' The', ' ceremony', ' was', ' attended', ' on', ' Spanish', ' country', ' City', ' Univ', '+', ' la', ' Cas', 'as', 'as', ',', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "GPT2 argmax:  ['\\n', ' show', ' was', ' place', ' at', ' the', ' 11', ',', ' 2017', '.', ' New', ' city', ' of', 'o', ',', ' Palace', ' a', 'ap', 'ul', 'co', ',', ' Mexico', ',', '\\n', ' ceremony', ' was', ' attended', ' on', ' Spanish', ' United', ' City', ' Univ', ' Plus', ' la', ' Cas', 'as', 'as', ',', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['The', ' awards', ' ceremony', ' took', ' place', ' on', ' March', ' 15', ',', ' 2009', ' in', ' the', ' Forum', ' Mund', 'o', ' Imperial', ',', ' Ac', 'ap', 'ul', 'co', ',', ' Guerrero', '.', ' The', ' ceremony', ' was', ' televised', ' in', ' the', ' Mexico', ' by', ' Canal', ' de', ' las', ' est', 'rell', 'as', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Y', 'ax', ' Patel', ' (', 'born', ' 29', ' May', ' 1999', ')', ' is', ' a', ' Canadian', ' cr', 'ick', 'eter', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' the', ' 2015', ',', ',', ' was', ' diagnosed', ' the', ' the', ' the', ' for', ' the', ' for', ' the', '', '20', '50', ' the', ' World', ' the', '.', '.', '.', '.', '.', '.', '.', '.', ',', '.', ' on', ' the', ' the', ' the', ' the', ' the', ' the', ' the', ' the', ' the', ' the']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 argmax:  [' the', ',', ',', ' the', ' will', ' named', ' the', ' the', \"'s\", ' top', ' for', ' the', ' World', ' World', '20', ' season', ' Championship', ' Rugby', '.', '.', ' the', ' United', '.', '.', '\\n', ' was', ' his', ' international', ' of', ' debut', ' in', ' the', ' May', ',', ',', ' and', ' the', \"'s\", ' South', ' United', 'in', 'ard', ' Islands', '.', ' and', ' the', ' Caribbean', ' Super', '50', ' tournament']\n",
      "Actual string:  ['In', ' October', ' 2019', ',', ' he', ' was', ' named', ' in', ' Canada', \"'s\", ' squad', ' for', ' the', ' 2019', '', '20', ' Regional', ' Super', '50', ' tournament', ' in', ' the', ' West', ' Indies', '.', ' He', ' made', ' his', ' List', ' A', ' debut', ' on', ' 8', ' November', ' 2019', ',', ' for', ' Canada', ' against', ' the', ' Le', 'ew', 'ard', ' Islands', ',', ' in', ' the', ' Regional', ' Super', '50']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' Sub', 'urban', ' Trust', ' and', ' Savings', ' Bank', ' Building', ' is', ' a', ' bank', ' building', ' at', ' 840', ' S', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' is', ',', ',', ',', ' Park', ' Park', ',', '\\n', '\\n', ' was', ' was', ' built', ' is', ',', ' the', ' the', ' the', ' of', ' of', ' the', ' of', ' building', ' building', ' building', ' building', ' the', ' the', ' the', ' the', ' the', ' the', ' the', ' with', ' with', ' with', ' with', ' is', ' is', ' is', ' is']\n",
      "GPT2 argmax:  [',', ',', ',', ' and', ' Park', ' Avenue', ' IL', '.', '\\n', ' is', ' the', ' in', ' 18', ' phases', ',', ' one', ' the', ' first', ' being', ' being', ' the', ' building', ' being', ' completed', ' in', ' 18', '.', ' the', ' in', ' May', ' 1', ',', ' 1926', '.', ' The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['Oak', ' Park', ' Avenue', ',', ' Oak', ' Park', ',', ' Illinois', '.', ' It', ' was', ' built', ' in', ' two', ' stages', ',', ' with', ' the', ' first', ' portion', ' of', ' the', ' building', ' being', ' built', ' in', ' 1925', ' and', ' opening', ' on', ' May', ' 1', ',', ' 1926', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Howard', ' Douglas', ' Grant', ' (', 'c', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', '\\n', '\\n', '\\n', ',', ',', ',', '\\n', '\\n', '\\n', '\\n', ' the', '\\n', 'bred', 'bred', '.', '.', ',', ',', ',', ',', ',', ',', ',', ',', ',', ',', ' the', ' the', ' and', ' the']\n",
      "GPT2 argmax:  ['.', ',', ' 18', ' 18', ',', ' 18', '\\n', '\\n', ' a', ' American', '-', 'orough', 'bred', ' racing', ' racing', ' series', 'ockey', '.', ' He', ' in', ' New', ',', ' Ohio', ',', ' in', ' was', ' racing', ' racing', 'ockey', ' career', 'hip', ' at', ' a', ' young', '-', 'year', '-', 'old', ' in', ' the', 'eling', ',', ' in', ' where', ' Virginia', '.', ' then', ' the', ' first', ' race']\n",
      "Actual string:  ['19', '39', ' ', ' August', ' 1', ',', ' 2018', ')', ' was', ' an', ' American', ' Th', 'orough', 'bred', ' horse', ' racing', ' j', 'ockey', '.', ' Born', ' in', ' Cincinnati', ',', ' Ohio', ',', ' he', ' began', ' his', ' j', 'ockey', ' apprentices', 'hip', ' as', ' a', ' seventeen', '-', 'year', '-', 'old', ' at', ' Whe', 'eling', ' Downs', ',', ' West', ' Virginia', ' and', ' won', ' his', ' first']\n",
      "\n",
      "\n",
      "Preceding string:  ['P', 'as', 'it', 'hee', ',', ' also', ' known', ' as', ',', ' is', ' a', ' retro', 'grade', ' irregular', ' satellite', ' of', ' Jupiter', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', \"'s\", ' a', ' that', ' a', ' group', ' of', ' scientists', ' that', ' that', ' University', ' of', ' of', ' the', ' by', ' by', '.', '.', '.', ',', '.', ',', '.', '.', ' the', ' the', ' the', ' the', ' the', ' to', ' to', ' to', ' to', ' to', ' of', ' of', ' of', ' of', ' of', ' to', ' to', ' to', ' to']\n",
      "GPT2 argmax:  [' is', ' a', ' that', ' a', ' team', ' of', ' researchers', ' who', ' the', ' University', ' of', ' California', ' at', ' by', ' Dr', ' W', '.', ' H', 'ppard', ',', ' the', '.', ' who', ' it', ' the', ' name', ' name', ' of', '...', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' was', ' discovered', ' by', ' a', ' team', ' of', ' astronomers', ' from', ' the', ' University', ' of', ' Hawaii', ' led', ' by', ' Scott', ' S', '.', ' She', 'ppard', ' in', ' 2001', ',', ' and', ' given', ' the', ' temporary', ' designation', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['On', ' 26', ' May', ' 2010', ',', ' at', ' least', ' seven', ' people', ' were', ' killed', ' in', ' a', ' bomb', ' blast', ' in', ' St', 'av', 'rop', 'ol', ',', ' Russia', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', ' the', ' the', ' hours', ' of', ' have', ' in', ' one', ' from', ',', ',', ',', ',', ' is', ' a', ' a', ',', ' the', ',', ' from', ' Turkey', ' Turkey', ' Turkey', '.', '.', ' and', ' was', ' the', ' a', '\\n', ',', '\\n', '\\n', '\\n', '\\n', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', '\"', 'A']\n",
      "GPT2 argmax:  [' the', ' one', ' people', ' were', ' killed', ' in', ' including', ' of', ' the', ' and', ' one', ' a', ' was', ' in', ' American', '.', ' according', ' a', ' is', ' the', '.', ' Azerbaijan', '.', '\\n', ' Russian', ' was', ' in', ' the', ' concert', ' in', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['At', ' least', ' 40', ' people', ' were', ' injured', ',', ' one', ' from', ' Moscow', ',', ' while', ' another', ' is', ' an', ' outsider', ',', ' and', ' another', ' from', ' Azerbaijan', ' or', ' Turkey', '.', ' The', ' blast', ' occurred', ' before', ' a', ' concert', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Elizabeth', ' H', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'A', '\\n', '\\n', '\\n', '\\n', ',', ',', ',', ',', ')', ',', ',', ',', ')', ')', ')', ')', ' a', ' a', ' a', ' and', ' a', ' a', ' in', ' in', ' and', ' and']\n",
      "GPT2 argmax:  ['\\n', ')', '\\n', ' (', 'Howard', ')', ',', ' 2016', '83', ')', ' March', ' 1', ',', ' 18', ')', ' p', ' a', ' member', 'ibrarian', ' at', ' a', 'ivist', ' of', ' in', ' the', ' Library', ' States', ' Library', ' the', ' Civil', ' part', 'th', ' century', '.', ' He', ' was', ' a', ' to', ' first', ' State', ' Library', 'ibrarian', ' in', ' 18', ' and', ' and', ' appointed', ' years', ' Texas', ' of']\n",
      "Actual string:  ['(', 'Howard', ')', ' West', ' (', 'March', ' 23', ',', ' 18', '73', ' ', ' January', ' 4', ',', ' 1948', '),', ' was', ' a', ' l', 'ibrarian', ' and', ' arch', 'ivist', ' active', ' in', ' the', ' United', ' States', ' during', ' the', ' early', ' 20', 'th', ' century', '.', ' West', ' was', ' appointed', ' the', ' Texas', ' State', ' L', 'ibrarian', ' in', ' 1918', ',', ' was', ' two', ' time', ' President']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' Gl', 'aser', ' coupling', ' is', ' a', ' type', ' of', ' coupling', ' reaction', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', \"'s\", ' a', ' the', ' the', ' the', ' of', 'yl', 'ic', ',', ' is', ' is', ' on', ' on', 'rous', 'rous', ',', ',', ')', ')', ')', ')', ')', ')', ')', ')', ')', ')', 'ide', 'ide', 'ide', 'ide', 'ide', 'ide', 'ide', 'ide']\n",
      "GPT2 argmax:  [' is', ' not', ' no', ' the', ' most', ' known', 'amin', 'ch', ' acid', ' molecule', ' the', ' the', ' on', ' the', 'id', 'al', '.', ' acet', ',', 'II', ')', ' and', ' and', ' copper', '(', 'II', ')', ' chloride', 'rom', 'ide', '.', ' is', 'hyd', ' copper', 'izing', ' called', ' sodium', '.', ' It', ' acet', ' of', ' the', ' base', ' form', ' is', ' a', '.', ' The', 'The', 'The']\n",
      "Actual string:  ['It', ' is', ' by', ' far', ' the', ' oldest', ' acet', 'yl', 'enic', ' coupling', ' and', ' is', ' based', ' on', ' cup', 'rous', ' salts', ' like', ' copper', '(', 'I', ')', ' chloride', ' or', ' copper', '(', 'I', ')', ' b', 'rom', 'ide', ' and', ' an', ' additional', ' oxid', 'ant', ' like', ' oxygen', '.', ' The', ' base', ' in', ' its', ' original', ' scope', ' is', ' ammonia', '.', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Amb', 'at', 'ond', 'raz', 'aka', ' District', ' is', ' a', ' district', ' in', ' the', ' Ala', 'ot', 'ra', '-', 'M', 'ang', 'oro', ' Region', ' of', ' Madagascar', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', ' a', ' is', ' a', ' city', ' of', ' H', ',', 'ond', 'raz', ',', ',', ',', ' is', ' is', ' of', ' of', ' of', ' of', ' of', ' the', ' population', ' in', ' in', ' was', ',', ',', ',', ',', '\\n', '\\n', 'The', 'The', 'The', 'R', 'R', 'R', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'is']\n",
      "GPT2 argmax:  [',', ',', ' in', ' city', ' of', ' K', 'man', ',', ',', ',', ',', ' It', ' town', ' is', ' been', ' estimated', ' of', ' about', ' which', ' is', ' capital', ' population', ' is', ' the', ' was', ' 1', ',', '000', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['Its', ' capital', ' is', ' the', ' town', ' of', ' Am', 'bat', 'ond', 'raz', 'aka', '.', ' The', ' district', ' has', ' an', ' area', ' of', ',', ' and', ' the', ' estimated', ' population', ' in', ' 2013', ' was', ' 324', ',', '610', '..', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['N', 'ash', 'ville', ' is', ' the', ' capital', ' and', ' most', ' populous', ' city', ' of', ' the', ' U', '.', 'S', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' of', ' the', '\\n', '\\n', \"'s\", ' a', ' of', ' of', ' of', ' of', ' County', ' of', ' of', ' of', ' of', ' County', ' County', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is', ' is']\n",
      "GPT2 argmax:  ['.', ' the', ',', '\\n', ' was', ' a', ' first', ' seat', ' of', ' the', ' County', ',', ' is', ' held', ' in', ' the', ' western', 'land', ' River', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['state', ' of', ' Tennessee', '.', ' It', ' is', ' the', ' county', ' seat', ' of', ' Davidson', ' County', ' and', ' is', ' located', ' on', ' the', ' Cumber', 'land', ' River', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['T', 'oon', 'erville', ' is', ' an', ' un', 'inc', 'orporated', ' community', ' in', ' Pike', ' County', ',', ' in', ' the', ' U', '.', 'S', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', '-', ' the', '\\n', '\\n', \"'s\", \"'s\", ' in', ' the', ' States', ' State', ' the', ',', ',', '\\xa0', '\\xa0', '\\xa0', '.', ',', 'km', 'km', ')', ',', ' of', ' of', ',', ',', '\\xa0', '\\xa0', '\\xa0', '\\xa0', '\\xa0', '\\xa0', '\\xa0', '\\xa0', '\\xa0', ' US', ' US', ' US', ' US', ' US', ' US', ' US', ' US']\n",
      "GPT2 argmax:  ['.', ' the', ',', '\\n', ' is', ' turn', ' in', ' the', ' States', '.', ' Kentucky', ' of', ' miles', ' miles', '\\xa0', 'about', ' about', ' km', ' km', 'mi', ')', ' of', ' of', ' the', ',', ' DC', ' state', \"'s\", ' largest', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['state', ' of', ' Kentucky', '.', ' It', ' in', ' located', ' in', ' United', ' States', ' -', ' some', ' 301', '\\xa0', 'mi', ' (', 'or', ' 48', '5', '\\xa0', 'km', ')', ' West', ' of', ' Washington', ',', ' the', ' country', \"'s\", ' capital', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['Mod', 'est', 'o', ' Junior', ' College', ' (', 'M', 'JC', ')', ' is', ' a', ' public', ' community', ' college', ' in', ' Modest', 'o', ',', ' California', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', \"'s\", ' a', ' of', ' the', ' Valley', \"'s\", \"'s\", \"'s\", ' in', ' Columbia', '.', '.', ',', ',', ',', ' Columbia', ' Columbia', ',', ' to', ' to', ' to', ' College', ' College', ' College', ' College', ' with', ' with', ' with', ' community', ' to', ' to', ',', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', '\\n', ' to', \"'s\", \"'s\", \"'s\", \"'s\"]\n",
      "GPT2 argmax:  [' is', ' not', ' of', ' the', \"'s\", ' Park', \"'s\", \"'s\", ' with', ' the', ' University', ' and', '\\n', ' is', ' is', ' a', ' the', ' College', ',', ' are', ' to', ' the', ' same', ' State', ' College', ' District', '.', ' with', ' the', ' other', ' community', ' colleges', ' colleges', '.', '\\n', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The']\n",
      "Actual string:  ['It', ' is', ' part', ' of', ' Yosemite', ' Community', ' College', ' District', ' along', ' with', ' Columbia', ' College', '.', ' MJ', 'C', ',', ' and', ' Columbia', ' College', ',', ' belong', ' to', ' the', ' California', ' Community', ' College', ' system', ' along', ' with', ' 112', ' other', ' public', ' community', ' colleges', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "\n",
      "\n",
      "Preceding string:  ['The', ' Whites', 'hell', ' Re', 'actor', ' No', '.', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "Predicted argmax:  ['The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', 'The', ' SHARES', '.', ' the', '-', '1', '\\n', '\\n', ' a', ' a', ' a', ' at', ' at', ' at', ' at', 'EC', '.', \"'s\", '.', '.', ' (', ' in', ' in', ' in', '.', ' was', ' was', ' was', ' was', ' was', ' was', ' was']\n",
      "GPT2 argmax:  ['.', '000', ' a', '3', '1', ',', ' or', ' the', ' \"', ' national', ' project', '.', ' in', ' the', '.', 'ON', ',', ' A', 'ide', ' facility', ' in', 'W', 'L', ')', ' in', ' the', ',', ' The', ' was', ' the', ' in', ' produce', ' the', ' feasibility', ' of', ' a', ' nuclear', '-', 'U', '-', '2', ' reactor', '.', ' could', ' the', ' existing', '-', ' reactor', 'ant', ' reactor']\n",
      "Actual string:  ['1', ',', ' or', ' WR', '-', '1', ',', ' was', ' a', ' Canadian', ' research', ' reactor', ' located', ' at', ' A', 'EC', 'L', \"'s\", ' Whites', 'hell', ' Laboratories', ' (', 'WN', 'RL', ')', ' in', ' Manitoba', '.', ' It', ' was', ' built', ' to', ' test', ' the', ' concept', ' of', ' a', ' C', 'AND', 'U', '-', 'type', ' reactor', ' that', ' replaced', ' the', ' heavy', ' water', ' cool', 'ant']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#compare against the actual strings from the batch\n",
    "for i in range(len(predicted_strings)):\n",
    "    print(\"Preceding string: \", tokenizer.batch_decode(temp[\"tokenized_first_sentence\"][i]))\n",
    "    print(\"Predicted argmax: \", tokenizer.batch_decode(predicted_ids[i]))\n",
    "    print(\"GPT2 argmax: \", tokenizer.batch_decode(gpt2_predicted_ids[i]))\n",
    "    print(\"Actual string: \", tokenizer.batch_decode(temp[\"tokenized_next_50_tokens\"][i]))\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoding the first element of batch\n",
    "tokenizer.decode(batch[\"tokenized_next_50_tokens\"][0]), tokenizer.decode(batch[\"tokenized_next_50_tokens\"][0]), tokenizer.decode(batch[\"tokenized_first_sentence\"][0][0]), tokenizer.decode(batch[\"tokenized_first_sentence\"][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"edit_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_output['edited_hidden_states'][0][0,7][0:10] - model(cat_example,output_hidden_states = True).hidden_states[0][0][7][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edit_embedding[0:10]\n",
    "result['editor_attention'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block calculates edit norm relative to the size of the current activations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "for batch_index in range(min(20,len(batch[\"tokenized_next_50_tokens\"]))):\n",
    "\n",
    "    #The tensor norm comes in an 8x13 matrix\n",
    "    edit_tensor = result[\"edit_vectors\"][batch_index].to(\"cpu\")\n",
    "    edit_tensor[:8,:, :] = edit_tensor[:8,:,:]/result[\"target_hidden_states\"][batch_index].norm(dim=2, keepdim=True) .to(\"cpu\")\n",
    "    edit_tensor_norm = edit_tensor.norm(dim=2)\n",
    "\n",
    "    # is this any better??\n",
    "    # attention_matrix = result['editor_attention'][batch_index].reshape(104).to(\"cpu\").reshape(13,8).permute(1,0)\n",
    "\n",
    "    # Detach and convert to numpy\n",
    "    edit_tensor_norm = edit_tensor_norm.detach().numpy()[0:stopping_index, :]\n",
    "\n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.imshow(edit_tensor_norm, cmap=\"hot\")\n",
    "\n",
    "    # Color the heatmap according to the entry sizes\n",
    "    heatmap.set_clim(vmin=np.min(0), vmax=np.max(edit_tensor_norm))\n",
    "    cbar = plt.colorbar(heatmap)\n",
    "    cbar.set_label(\"Entry Sizes\")\n",
    "\n",
    "    # Add labels to the x and y axes\n",
    "    ax.set_xticks(np.arange(13))\n",
    "    ax.set_yticks(np.arange(8))\n",
    "    ax.set_xticklabels(np.arange(13))\n",
    "    ax.set_yticklabels(np.arange(8))\n",
    "\n",
    "    # Rotate the x-axis labels\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Add a title\n",
    "    plt.title(\"Edit Norm / Target Norm Heatmap\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    print(tokenizer.batch_decode(batch[\"tokenized_next_50_tokens\"][batch_index][0:8]))\n",
    "    print(tokenizer.batch_decode(batch[\"tokenized_first_sentence\"][batch_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "for batch_index in range(min(10,len(batch[\"tokenized_next_50_tokens\"]))):\n",
    "\n",
    "    #The tensor norm comes in an 8x13 matrix\n",
    "    edit_tensor = result[\"edit_vectors\"][batch_index].to(\"cpu\")\n",
    "    edit_tensor_norm = edit_tensor.norm(dim=2)\n",
    "\n",
    "    # is this any better??\n",
    "    # attention_matrix = result['editor_attention'][batch_index].reshape(104).to(\"cpu\").reshape(13,8).permute(1,0)\n",
    "\n",
    "    # Detach and convert to numpy\n",
    "    edit_tensor_norm = edit_tensor_norm.detach().numpy()[0:stopping_index, :]\n",
    "\n",
    "    # Create the heatmap\n",
    "    fig, ax = plt.subplots()\n",
    "    heatmap = ax.imshow(edit_tensor_norm, cmap=\"hot\")\n",
    "\n",
    "    # Color the heatmap according to the entry sizes\n",
    "    heatmap.set_clim(vmin=np.min(0), vmax=np.max(edit_tensor_norm))\n",
    "    cbar = plt.colorbar(heatmap)\n",
    "    cbar.set_label(\"Entry Sizes\")\n",
    "\n",
    "    # Add labels to the x and y axes\n",
    "    ax.set_xticks(np.arange(13))\n",
    "    ax.set_yticks(np.arange(8))\n",
    "    ax.set_xticklabels(np.arange(13))\n",
    "    ax.set_yticklabels(np.arange(8))\n",
    "\n",
    "    # Rotate the x-axis labels\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    # Add a title\n",
    "    plt.title(\"Edit Norm Heatmap\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "    print(tokenizer.batch_decode(batch[\"tokenized_next_50_tokens\"][batch_index][0:8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "stopping_index = 8\n",
    "\n",
    "for head_index in range(min(hypernetwork.editor_model.config.num_editing_heads,10)):\n",
    "    for batch_index in range(3):\n",
    "\n",
    "        # Reshape the tensor into an 8x13 matrix\n",
    "        attention_matrix = result[\"editor_attention\"][batch_index][head_index].reshape(8, 13).to(\"cpu\")\n",
    "\n",
    "        # is this any better??\n",
    "        # attention_matrix = result['editor_attention'][batch_index].reshape(104).to(\"cpu\").reshape(13,8).permute(1,0)\n",
    "\n",
    "        # Detach and convert to numpy\n",
    "        attention_matrix = attention_matrix.detach().numpy()\n",
    "\n",
    "        # Create the heatmap\n",
    "        fig, ax = plt.subplots()\n",
    "        heatmap = ax.imshow(attention_matrix, cmap=\"hot\")\n",
    "\n",
    "        # Color the heatmap according to the entry sizes\n",
    "        heatmap.set_clim(vmin=np.min(attention_matrix), vmax=np.max(attention_matrix))\n",
    "        cbar = plt.colorbar(heatmap)\n",
    "        cbar.set_label(\"Entry Sizes\")\n",
    "\n",
    "        # Add labels to the x and y axes\n",
    "        ax.set_xticks(np.arange(13))\n",
    "        ax.set_yticks(np.arange(8))\n",
    "        ax.set_xticklabels(np.arange(13))\n",
    "        ax.set_yticklabels(np.arange(8))\n",
    "\n",
    "        # Rotate the x-axis labels\n",
    "        plt.xticks(rotation=90)\n",
    "\n",
    "        # Add a title\n",
    "        plt.title(\"Editor Attention Heatmap\")\n",
    "\n",
    "        # Show the plot\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        print(tokenizer.batch_decode(batch[\"result_text\"][batch_index]))\n",
    "        print(tokenizer.batch_decode(batch[\"editor_tokens\"][batch_index][8:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.norm(\n",
    "#     result[\"edit_vectors\"][batch_index][:stopping_index, :, :].to(\"cpu\"), dim=[0, 2]\n",
    "# )  # looks better now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes: It seems, currently, like we are not giving sufficient incentive to intervene at the lowest possible layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"edit_vectors\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edit_tensor = result[\"edit_vectors\"][batch_index].reshape(8, 13, -1).to(\"cpu\")\n",
    "edit_tensor_norm = edit_tensor.norm(dim=2)\n",
    "edit_tensor_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edited_states = torch.stack(result['edited_hidden_states'],dim = 2)\n",
    "edited_states.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    print(edited_states[0][i][0].norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    torch.norm(result['target_hidden_states'][0][6][0]),\n",
    "    torch.norm(result['target_hidden_states'][0][0][0]),\n",
    "    edited_states[0][6][0].norm(),\n",
    "    edited_states[0][2][0].norm()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(hypernetwork.state_dict(), \"/root/aiplay-1/hypernetworks/hypernetwork.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
